{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1552144,"sourceType":"datasetVersion","datasetId":915988},{"sourceId":11111439,"sourceType":"datasetVersion","datasetId":6829806}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis - Extract Features\n- Create sentiment features\n- Train from scratch methods using traditional NLP/NLTK\n- Also use pre trained models and compare all to randomly guessing sentiment\n- Output training and testing sets","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers twython langdetect pycountry nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:26:56.251333Z","iopub.execute_input":"2025-03-22T00:26:56.251825Z","iopub.status.idle":"2025-03-22T00:27:00.396031Z","shell.execute_reply.started":"2025-03-22T00:26:56.251786Z","shell.execute_reply":"2025-03-22T00:27:00.394847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:00.397774Z","iopub.execute_input":"2025-03-22T00:27:00.398160Z","iopub.status.idle":"2025-03-22T00:27:00.402472Z","shell.execute_reply.started":"2025-03-22T00:27:00.398121Z","shell.execute_reply":"2025-03-22T00:27:00.401477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get dataset that had entropies and languages identified\ndf = pd.read_pickle('/kaggle/input/reviews-analyzer-dataset/df_w_langs_entropies.pkl')\nprint(df.columns)\ndf.rename(columns={'reviews': 'review', 'language': 'lang_code'}, inplace=True)\nprint(f\"Total entries = {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:00.404612Z","iopub.execute_input":"2025-03-22T00:27:00.404901Z","iopub.status.idle":"2025-03-22T00:27:01.216432Z","shell.execute_reply.started":"2025-03-22T00:27:00.404876Z","shell.execute_reply":"2025-03-22T00:27:01.215507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.rating.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:01.217652Z","iopub.execute_input":"2025-03-22T00:27:01.217906Z","iopub.status.idle":"2025-03-22T00:27:01.234123Z","shell.execute_reply.started":"2025-03-22T00:27:01.217885Z","shell.execute_reply":"2025-03-22T00:27:01.233182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pycountry\nimport warnings\nfrom tqdm import tqdm\n\ndef get_language_name(code):\n    \"\"\"Convert ISO 639-1 code to full language name using pycountry.\"\"\"\n    if code == 'unknown':\n        return 'Unknown'\n    try:\n        # Handle cases like 'zh-cn' by taking the first part\n        code = code.split('-')[0]\n        lang = pycountry.languages.get(alpha_2=code)\n        return lang.name.lower() if lang else code\n    except AttributeError:\n        return code\n\n# Do some basic processing for readability\n\ntqdm.pandas(desc='Detecting Languages')\ndf['language'] = df['lang_code'].progress_apply(get_language_name)\nprint(df.rating.value_counts())\nprint(df.language.value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:01.235066Z","iopub.execute_input":"2025-03-22T00:27:01.235325Z","iopub.status.idle":"2025-03-22T00:27:05.820476Z","shell.execute_reply.started":"2025-03-22T00:27:01.235303Z","shell.execute_reply":"2025-03-22T00:27:05.819530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Remove Gibberish with Gibberish Detector Model\n- Load and apply gibberish detector","metadata":{}},{"cell_type":"code","source":"def label_sentiment(entry):\n    if entry > 3:\n        return 'positive'\n    elif entry == 3:\n        return 'neutral'\n    elif entry < 3:\n        return 'negative'\n\n# Function to vectorize with progress bar\ndef vectorize_with_progress(reviews, desc):\n    with tqdm(total=1, desc=desc) as pbar:\n        matrix = vectorizer.fit_transform(reviews)\n        freq = matrix.toarray().sum(axis=0)\n        pbar.update(1)\n    return freq, vectorizer.get_feature_names_out()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:05.821301Z","iopub.execute_input":"2025-03-22T00:27:05.821589Z","iopub.status.idle":"2025-03-22T00:27:05.827115Z","shell.execute_reply.started":"2025-03-22T00:27:05.821565Z","shell.execute_reply":"2025-03-22T00:27:05.826005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grab NLTK stuff\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# What languages are available? \n#print(stopwords.fileids())\n\n# Get basic NLTK stuff\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n# Only run once to get lexicon\n!unzip /usr/share/nltk_data/sentiment/vader_lexicon.zip\n\n# What were the top languages we had in our course reviews?\nnltk_set = set(stopwords.fileids())\ncourse_set = set(df.language)\noverlapping_languages = list(nltk_set.intersection(course_set))\n\n# Get stop words for all languages in dict form\nstop_words_dict = {lang: set(stopwords.words(lang)) for lang in overlapping_languages}\n\n# Combine stop words from all languages\nall_stop_words = set()\nfor lang in overlapping_languages:\n    try:\n        lang_stop_words = set(stopwords.words(lang))\n        all_stop_words.update(lang_stop_words)\n    except ValueError as e:\n        print(f\"No stop words for {lang}: {e}\")\n\nprint(f\"Total stop words: {len(all_stop_words)} (sample: {list(all_stop_words)[:5]})\")\n\n# Load VADER words instead of listing by hand\nvader_lexicon_path = '/kaggle/working/vader_lexicon/vader_lexicon.txt'\nvader_scores = pd.read_csv(vader_lexicon_path, sep='\\t', header=None, \n                           names=['word', 'score', 'std_dev', 'count'])\nvader_dict = dict(zip(vader_scores['word'], vader_scores['score']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:05.827918Z","iopub.execute_input":"2025-03-22T00:27:05.828185Z","iopub.status.idle":"2025-03-22T00:27:07.530297Z","shell.execute_reply.started":"2025-03-22T00:27:05.828161Z","shell.execute_reply":"2025-03-22T00:27:07.529179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explore existing bi-grams and tri-grams\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Split by sentiment\ndf['sentiment_label'] = df['rating'].apply(label_sentiment)\npos_reviews = df[df['sentiment_label'] == 'positive']['review']\nneg_reviews = df[df['sentiment_label'] == 'negative']['review']\n\n# Handle NaN/non-string values\npos_reviews = pos_reviews.fillna('')  # Replace NaN with empty string\npos_reviews = pos_reviews.astype(str)  # Ensure all are strings\nneg_reviews = neg_reviews.fillna('').astype(str)\n\n# Extract top n-grams\nvectorizer = CountVectorizer(ngram_range=(2, 3), stop_words=list(all_stop_words), max_features=20)\n\n# Extract top n-grams with progress\nprint(\"Extracting n-grams...\")\npos_ngrams_freq, pos_ngrams_names = vectorize_with_progress(pos_reviews, \"Processing Positive Reviews\")\npos_ngrams_dict = dict(zip(pos_ngrams_names, pos_ngrams_freq))\n\nif len(neg_reviews) > 0:\n    neg_ngrams_freq, neg_ngrams_names = vectorize_with_progress(neg_reviews, \"Processing Negative Reviews\")\n    neg_ngrams_dict = dict(zip(neg_ngrams_names, neg_ngrams_freq))\nelse:\n    neg_ngrams_dict = {}\n    print(\"No negative reviews to process.\")\n\nprint(\"Top Positive N-grams:\", pos_ngrams_dict)\nprint(\"Top Negative N-grams:\", neg_ngrams_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:27:07.533454Z","iopub.execute_input":"2025-03-22T00:27:07.533770Z","iopub.status.idle":"2025-03-22T00:28:23.480585Z","shell.execute_reply.started":"2025-03-22T00:27:07.533742Z","shell.execute_reply":"2025-03-22T00:28:23.478765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assign polarity scores\ndef assign_polarity(ngrams_dict, base_score, adjustment=0.1):\n    polarity_dict = {}\n    for ngram, freq in ngrams_dict.items():\n        extra = (freq - 1) * adjustment if freq > 1 else 0\n        polarity_dict[ngram] = base_score + extra\n    return polarity_dict\n\npositive_ngrams = assign_polarity(pos_ngrams_dict, base_score=2.0, adjustment=0.1)\nnegative_ngrams = assign_polarity(neg_ngrams_dict, base_score=-2.0, adjustment=-0.1)\n\nprint(\"Positive N-grams with Polarity:\", positive_ngrams)\nprint(\"Negative N-grams with Polarity:\", negative_ngrams)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:28:23.481924Z","iopub.execute_input":"2025-03-22T00:28:23.482170Z","iopub.status.idle":"2025-03-22T00:28:23.489700Z","shell.execute_reply.started":"2025-03-22T00:28:23.482150Z","shell.execute_reply":"2025-03-22T00:28:23.488815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"negation_words = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:28:23.490557Z","iopub.execute_input":"2025-03-22T00:28:23.490815Z","iopub.status.idle":"2025-03-22T00:28:23.508533Z","shell.execute_reply.started":"2025-03-22T00:28:23.490793Z","shell.execute_reply":"2025-03-22T00:28:23.507480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update feature extraction with VADER scores\ndef extract_features_with_vader(text):\n    tokens = word_tokenize(text.lower())\n    #tokens = [w for w in tokens if w not in list(all_stop_words)]\n    # Loop with progress bar\n    filtered_tokens = []\n    for w in tqdm(tokens, desc=\"Filtering Tokens\", leave=False):\n        if w not in list(all_stop_words):\n            filtered_tokens.append(w)\n    tokens = filtered_tokens\n    # Unigram features\n    pos_count = 0\n    neg_count = 0\n    negated_pos_count = 0\n    negated_neg_count = 0\n    polarity = 0\n    negated = False\n    \n    # N-gram features\n    pos_ngram_count = 0\n    neg_ngram_count = 0\n    \n    # Generate bigrams and trigrams\n    bigrams = [' '.join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n    trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens)-2)]\n    ngrams = bigrams + trigrams\n    \n    # N-gram scoring\n    for ngram in ngrams:\n        if ngram in positive_ngrams:\n            pos_ngram_count += 1\n            polarity += positive_ngrams[ngram]\n        elif ngram in negative_ngrams:\n            neg_ngram_count += 1\n            polarity += negative_ngrams[ngram]\n    \n    # Unigram scoring with negation\n    for i, token in enumerate(tokens):\n        if token in negation_words:\n            negated = True\n            continue\n        \n        score = vader_dict.get(token, 0)\n        if negated and token not in {'.', ',', ';', '!'}:\n            if score > 0:\n                negated_pos_count += 1\n                polarity += score * -0.5\n            elif score < 0:\n                negated_neg_count += 1\n                polarity += score * -0.5\n            negated = False\n        else:\n            if score > 0:\n                pos_count += 1\n                polarity += score\n            elif score < 0:\n                neg_count += 1\n                polarity += score\n        \n        if token in {'.', ',', ';', '!'}:\n            negated = False\n    \n    exclamations = text.count('!')\n    uppercase_ratio = sum(c.isupper() for c in text) / len(text) if len(text) > 0 else 0\n    \n    return pd.Series({\n        'pos_word_count': pos_count,\n        'neg_word_count': neg_count,\n        'negated_pos_count': negated_pos_count,\n        'negated_neg_count': negated_neg_count,\n        'pos_ngram_count': pos_ngram_count,\n        'neg_ngram_count': neg_ngram_count,\n        'polarity_score': polarity,\n        'exclamation_count': exclamations,\n        'uppercase_ratio': uppercase_ratio\n    })\n\n\ntqdm.pandas(desc='Extracting features...')\n#df = df.head().copy()\nfeatures = df['review'].progress_apply(extract_features_with_vader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:08:23.171178Z","iopub.execute_input":"2025-03-22T02:08:23.171584Z","iopub.status.idle":"2025-03-22T02:08:23.318275Z","shell.execute_reply.started":"2025-03-22T02:08:23.171549Z","shell.execute_reply":"2025-03-22T02:08:23.317448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:09:09.381972Z","iopub.execute_input":"2025-03-22T02:09:09.382335Z","iopub.status.idle":"2025-03-22T02:09:09.387958Z","shell.execute_reply.started":"2025-03-22T02:09:09.382309Z","shell.execute_reply":"2025-03-22T02:09:09.386952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport os\nimport pickle\nimport zipfile\n\n#df['sentiment'] = df['rating'].apply(lambda x: 1 if x >= 4 else (0 if x <= 2 else None))\nfeatures['sentiment'] = df['rating'].apply(lambda x: 2 if x >= 4 else (1 if x == 3 else 0))\n#data = data.dropna(subset=['sentiment'])  # Drop neutral (3)\n\n# Feature columns\nfeature_cols = ['pos_word_count', 'neg_word_count', 'negated_pos_count', 'negated_neg_count',\n                'pos_ngram_count', 'neg_ngram_count', 'polarity_score', 'exclamation_count', 'uppercase_ratio']\nX = pd.concat([features[feature_cols], df['review']], axis=1)\ny = features['sentiment']\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n\n# Save as pickle files\noutput_dir = '/kaggle/working/'  # Kaggle default; adjust for local (e.g., './data/')\nos.makedirs(output_dir, exist_ok=True)\n\nfiles_to_save = {\n    'X_train.pkl': X_train,\n    'X_test.pkl': X_test,\n    'y_train.pkl': y_train,\n    'y_test.pkl': y_test\n}\n\nprint(\"Saving datasets as pickle files...\")\nfor filename, obj in tqdm(files_to_save.items(), desc=\"Saving Pickle Files\"):\n    with open(os.path.join(output_dir, filename), 'wb') as f:\n        pickle.dump(obj, f)\n\nprint(f\"Saved pickle files to {output_dir}: {list(files_to_save.keys())}\")\n\n# Zip the pickle files\nzip_filename = os.path.join(output_dir, 'sentiment_data.zip')\nprint(\"Zipping files...\")\nwith zipfile.ZipFile(zip_filename, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    for filename in tqdm(files_to_save.keys(), desc=\"Adding Files to Zip\"):\n        zipf.write(os.path.join(output_dir, filename), arcname=filename)\n\nprint(f\"Saved and zipped files to {zip_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:22:41.619098Z","iopub.execute_input":"2025-03-22T02:22:41.619494Z","iopub.status.idle":"2025-03-22T02:22:41.646412Z","shell.execute_reply.started":"2025-03-22T02:22:41.619460Z","shell.execute_reply":"2025-03-22T02:22:41.645472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**NOTE:** Looks like we lose like 600 reviews in the inner join? Maybe we dump the course info if not needed.","metadata":{}},{"cell_type":"code","source":"'''\nfrom transformers import BartTokenizer, BartForConditionalGeneration\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n\ndef summarize(text):\n    inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors='pt')\n    summary_ids = model.generate(inputs['input_ids'], max_length=50, min_length=10, length_penalty=2.0, num_beams=4)\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nreal_reviews['summary'] = real_reviews['review_text'].apply(summarize)\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:28:23.648993Z","iopub.status.idle":"2025-03-22T00:28:23.649444Z","shell.execute_reply":"2025-03-22T00:28:23.649238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom transformers import pipeline\nsentiment_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\nreal_reviews['sentiment'] = real_reviews['review_text'].apply(lambda x: sentiment_classifier(x)[0]['label'])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T00:28:23.650418Z","iopub.status.idle":"2025-03-22T00:28:23.650837Z","shell.execute_reply":"2025-03-22T00:28:23.650654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### '''\nreal_reviews['rating_sentiment'] = real_reviews['rating'].apply(lambda x: 'POSITIVE' if x >= 4 else 'NEGATIVE')\naccuracy = (real_reviews['sentiment'] == real_reviews['rating_sentiment']).mean()\nprint(f\"Sentiment Accuracy (using ratings): {accuracy:.2f}\")\n''''''","metadata":{"execution":{"iopub.status.busy":"2025-03-22T00:28:23.651897Z","iopub.status.idle":"2025-03-22T00:28:23.652315Z","shell.execute_reply":"2025-03-22T00:28:23.652132Z"}}}]}