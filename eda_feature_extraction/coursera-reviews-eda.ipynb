{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1552144,"sourceType":"datasetVersion","datasetId":915988}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Coursera Reviews EDA\n- Explore the course metadata and explore relationships between features with rating as the target\n- Identify potentially interesting features to pair with sentiment\n- Explore word distributions, keywords, etc in reviews.","metadata":{}},{"cell_type":"code","source":"!pip -q install datasets transformers huggingface_hub langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T19:00:33.133740Z","iopub.execute_input":"2025-03-09T19:00:33.134022Z","iopub.status.idle":"2025-03-09T19:00:40.098316Z","shell.execute_reply.started":"2025-03-09T19:00:33.134000Z","shell.execute_reply":"2025-03-09T19:00:40.097471Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport scipy.stats as stats\nimport re\nimport nltk\nfrom nltk.corpus import words\nfrom collections import Counter\nimport math\nimport unicodedata\nimport time\nfrom tqdm import tqdm\nimport pickle\nfrom langdetect import detect\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T19:00:40.099531Z","iopub.execute_input":"2025-03-09T19:00:40.099764Z","iopub.status.idle":"2025-03-09T19:00:47.033806Z","shell.execute_reply.started":"2025-03-09T19:00:40.099741Z","shell.execute_reply":"2025-03-09T19:00:47.032911Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('words')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"courses_df = pd.read_csv('/kaggle/input/course-reviews-on-coursera/Coursera_courses.csv')\ncourses_df.head(3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T18:57:35.230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_df = pd.read_csv('/kaggle/input/course-reviews-on-coursera/Coursera_reviews.csv')\nreviews_df.head(3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T18:57:35.230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_df.reviews","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\ncounts, bins, patches = plt.hist(reviews_df['rating'], \n                                 bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n                                 edgecolor='black')\n    \nplt.xticks([1, 2, 3, 4, 5])\n\nfor count, x in zip(counts, [1, 2, 3, 4, 5]):\n    plt.text(x, count, f'{int(count)/len(reviews_df):.1%}', ha='center', va='bottom')\n    \nplt.xlabel('Course Rating')\nplt.ylabel('Count')\nplt.title('Distribution of Coursera Course Ratings')\nplt.grid(True, alpha=0.3)\n\n# Ensure y-axis starts at 0 and has some padding on top\nplt.margins(y=0.1)\nplt.tight_layout()\nplt.savefig('rating_hist.png')\n# Display the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_monthly_avg(df, dates, ratings):\n    # Plot average ratings over time\n    reviews_df[dates] = pd.to_datetime(reviews_df[dates])\n    \n    reviews_df['year_month'] = reviews_df[dates].dt.to_period('M')\n    monthly_avg = reviews_df.groupby('year_month')[ratings].mean().reset_index()\n    monthly_avg['year_month'] = monthly_avg['year_month'].dt.to_timestamp()\n    \n    # Plot the monthly average\n    plt.figure(figsize=(12, 6))\n    plt.plot(monthly_avg['year_month'], monthly_avg['rating'], \n             marker='o', linestyle='-', linewidth=1.5)\n    \n    # Plot overall average\n    plt.hlines(y=monthly_avg[ratings].mean(), \n               xmin=monthly_avg['year_month'].min(), \n               xmax=monthly_avg['year_month'].max(), \n               colors='g', linestyles='dotted', label='Avg. Rating')\n    \n    plt.xlabel('Date')\n    plt.ylabel('Average Course Rating')\n    plt.title('Average Ratings Over Time (Monthly)')\n    plt.grid(True, alpha=0.3)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    plt.legend()\n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    plt.savefig('rating_over_time.png')\n    # Show the plot\n    plt.show()\n\nplot_monthly_avg(reviews_df, 'date_reviews', 'rating')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_top_categories(df, column_name, n=20):\n\n    uni_counts = df[column_name].value_counts().head(n)\n    \n    # Create horizontal bar chart\n    plt.figure(figsize=(10, 8))\n    uni_counts.plot(kind='barh')\n    \n    plt.xlabel('Number of Occurrences')\n    plt.ylabel('Universities')\n    plt.title(f'Top {n} Most Frequent Universities')\n    plt.grid(True, alpha=0.3, axis='x')\n    \n    # Invert y-axis to have highest count at top\n    plt.gca().invert_yaxis()\n    \n    # Add counts on the bars\n    for i, v in enumerate(uni_counts):\n        plt.text(v, i, str(v), va='center')\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.savefig('top_universities.png')\n    plt.show()\n\nplot_top_categories(courses_df, 'institution')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Merge the dataframes","metadata":{}},{"cell_type":"code","source":"merged_df = pd.merge(courses_df, reviews_df, \n                    on='course_id', \n                    how='inner')  # 'inner' keeps only matching records","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T18:57:35.230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate average ratings per university and counts for each\navg_ratings = merged_df.groupby('institution')['rating'].mean()\nrating_counts = merged_df.groupby('institution')['rating'].count()\n\nuni_stats = pd.DataFrame({\n    'avg_rating': avg_ratings,\n    'n_ratings': rating_counts\n})\n\n# Find outliers with IQR\nQ1 = uni_stats['avg_rating'].quantile(0.25)\nQ3 = uni_stats['avg_rating'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = uni_stats[(uni_stats['avg_rating'] < lower_bound) | \n                    (uni_stats['avg_rating'] > upper_bound)].reset_index()\n\ndef get_hi_low(row, hi, low):\n    if row['avg_rating'] > hi:\n        return 'High'\n    elif row['avg_rating'] < low:\n        return 'Low'\n    else:\n        return \"Normal\"\n\noutliers['hi_lo'] = outliers.apply(lambda row: get_hi_low(row, upper_bound, lower_bound), axis=1)\n\nprint(\"Potential Outliers (using IQR method):\")\nif not outliers.empty:\n    #print(outliers)\n    # Construct Markdown table manually\n    md_table = f\"| {'Institution'} | Average Rating | Number of Ratings |\\n\"\n    md_table += \"|------------|----------------|-------------------|\\n\"\n    for _, row in outliers.iterrows():\n        md_table += f\"| {row['institution']} | {row['avg_rating']:.2f} | {row['n_ratings']} | {row['hi_lo']} |\\n\"\n    md_table += f\"| **Overall Avg** | **{uni_stats['avg_rating'].mean():.2f}** | **{len(merged_df)}** | N/A |\\n\"\n    print(md_table)\nelse:\n    print(\"No outliers detected.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Potential Average Rating Outliers (using IQR method):\n| Institution | Average Rating | Number of Ratings | High/Low |\n|------------|----------------|-------------------| ------- |\n| Advancing Women in Product | 4.33 | 9 | Low |\n| ESCP Business School | 4.31 | 153 | Low |\n| IE Business School | 4.01 | 82 | Low |\n| LearnQuest | 4.32 | 246 | Low |\n| New York Institute of Finance | 3.40 | 368 | Low |\n| Novosibirsk State University  | 4.03 | 195 | Low |\n| Saint Petersburg State University | 3.33 | 204 | Low |\n| University of New Mexico | 1.00 | 6 | Low |\n| Yandex | 3.42 | 290 | Low |\n| **Overall Avg** | **4.65** | **1454711** | N/A |","metadata":{}},{"cell_type":"markdown","source":"#### Cleaning the Reviews - Word Counts + Entropy","metadata":{}},{"cell_type":"code","source":"# Find entries with no review\nprint(f\"There are {sum(merged_df.reviews.isna())} entries with no review text\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = merged_df.dropna(subset=['reviews'])\nprint(f\"Dropped {len(merged_df) - len(df)} entries with no reviews\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df = df[df['reviews'].str.len() < 10]\n\nprint(f\"There are {len(df[df['reviews'].str.len() < 5])} entries with < 5 characters\")\nprint(f\"There are {len(df[df['reviews'].str.len() < 10])} entries with < 10 characters\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We notice that a lot of the remaining non null entries have very short reviews. Let's look at some of the entries where there are less than 4 characters. I picked 4 since the word \"good\" comes up quite frequently.","metadata":{}},{"cell_type":"code","source":"df[df['reviews'].str.len() < 4].head(10).reviews","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we limit it to 10 characters, we see reviews like \"its good\" or \"good\" so this seems like a reasonable limit. There may still be entries with nonsensical words.","metadata":{}},{"cell_type":"code","source":"df[df['reviews'].str.len() < 10].head(8).reviews","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Review length (word count)\nword_count_df = df.copy()\nword_count_df['word_count'] = word_count_df['reviews'].apply(lambda x: len(str(x).split()))\nprint(\"\\nReview Word Count Stats:\")\nprint(df['word_count'].describe().to_markdown())\n\nplt.figure(figsize=(10, 6))\nsns.histplot(word_count_df['word_count'], bins=10)\nplt.title('Distribution of Review Word Counts')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This plot is not very meaningful, need to cut the outliers to get better idea","metadata":{}},{"cell_type":"code","source":"def plot_word_count_distribution(original_df, review_col='reviews', percentile_cutoff=95, sample_size=5):\n\n    df = original_df.copy()\n    df['word_count'] = df[review_col].apply(lambda x: len(str(x).split()))\n\n    # Set word count cutoff to a given percentile. Defult to 95\n    cutoff = np.percentile(df['word_count'], percentile_cutoff)\n    print(f\"\\n{percentile_cutoff}th Percentile Cutoff: {cutoff:.0f} words\")\n\n    main_range = df['word_count'][df['word_count'] <= cutoff]\n    outliers = df[df['word_count'] > cutoff]\n    \n    # Plot histogram for main range\n    plt.figure(figsize=(10, 6))\n    sns.histplot(main_range, bins=30)\n    plt.title(f'Distribution of Word Counts (Up to {cutoff:.0f} Words)')\n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('word_count_hist.png')\n    plt.show()\n\n    # Make output df of outliers and print to markdown\n    if not outliers.empty:\n        \n        outlier_sample = outliers[[review_col, 'word_count']].head(sample_size)\n        \n        md_table = \"| Review Text | Word Count |\\n\"\n        md_table += \"|-------------|------------|\\n\"\n        for _, row in outlier_sample.iterrows():\n            # Truncate long text for display (optional)\n            text = row[review_col] if len(row[review_col]) < 50 else row[review_col][:47] + \"...\"\n            md_table += f\"| {text} | {row['word_count']} |\\n\"\n        \n        print(\"\\nSample of Outlier Reviews:\")\n        print(md_table)\n\n\nplot_word_count_distribution(df, percentile_cutoff=97.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sample of Outlier Reviews:\n| Review Text | Word Count |\n|-------------|------------|\n| This is an extremely basic course. Machine lear... | 152 |\n| I just started week 3 , I have to admit that It... | 93 |\n| This course is absolute garbage.  You get no fe... | 107 |\n| Have to give a star so I will give it one.  Oth... | 131 |\n| I would rename this course as Programming Octav... | 125 |","metadata":{}},{"cell_type":"markdown","source":"Next, we'll attempt to define a meaningful review as one with low entropy. Higher entropy suggests a more random distribution of characters, which is more likely to be gibberish over structured language.\n\n**NOTE**: We'll use unicodedata.category(c).startswith('L') to make sure the words have any unicode characters not just those in english alphabet. Additionally, chinese apparently can often have higher entropy, so we need to check if a review is in chinese and add seprate conditions. The same applies for cyrilic apparently.\n\n","metadata":{}},{"cell_type":"code","source":"def calculate_entropy(text):\n    \"\"\"Calculate Shannon entropy of the text to detect randomness.\"\"\"\n    if not text:\n        return 0\n    if not isinstance(text, str) or pd.isna(text):\n        return 0  # Return 0 for NaN or non-string values\n    text = str(text).lower()\n    length = len(text)\n    if length == 0:  # Handle empty strings\n        return 0\n    char_counts = Counter(text)\n    entropy = -sum((count/length) * math.log2(count/length) for count in char_counts.values())\n    return entropy\n\n# Check to see if text has chinese or cyrilic since they can have higher entropy\ndef has_chinese(text):\n    \"\"\"Check if text contains Chinese characters.\"\"\"\n    return any('\\u4e00' <= char <= '\\u9fff' for char in text)\n\ndef has_cyrillic(text):\n    \"\"\"Check if text contains Cyrillic characters.\"\"\"\n    return any('\\u0400' <= char <= '\\u04ff' for char in text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(merged_df['reviews'].apply(calculate_entropy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_entropy_distribution(test_df, review_col='reviews', \n                             base_entropy_english=4.5, entropy_scale=0.5,\n                             max_entropy_chinese=7.5, max_entropy_other=5.5):\n    df = test_df.copy()\n\n    \n    # Calculate entropies and lengths\n    df['entropy'] = df[review_col].apply(calculate_entropy)\n    df['length'] = df[review_col].apply(lambda x: len(str(x)))\n    \n    # Separate by language for analysis\n    df['is_chinese'] = df[review_col].apply(lambda x: any('\\u4e00' <= c <= '\\u9fff' for c in str(x)))\n    df['is_cyrillic'] = df[review_col].apply(lambda x: any('\\u0400' <= c <= '\\u04ff' for c in str(x)))\n    \n    # Compute dynamic English threshold for mean length (for reference)\n    mean_length = df['length'].mean()\n    max_entropy_english_mean = base_entropy_english + (mean_length / 100) * entropy_scale\n    \n    # Stats\n    print(\"Entropy Statistics:\")\n    print(df['entropy'].describe())\n    print(f\"\\nMean Review Length: {mean_length:.0f} characters\")\n    print(f\"Scaled max_entropy_english at mean length: {max_entropy_english_mean:.2f}\")\n    \n    # Plot histogram\n    plt.figure(figsize=(12, 6))\n    sns.histplot(df['entropy'], bins=50, color='blue', label='Entropy Distribution')\n    \n    # Add threshold lines\n    plt.axvline(x=base_entropy_english, color='red', linestyle='--', \n                label=f'Base English Threshold ({base_entropy_english})')\n    plt.axvline(x=max_entropy_english_mean, color='orange', linestyle='--', \n                label=f'Scaled English at Mean Length ({max_entropy_english_mean:.2f})')\n    plt.axvline(x=max_entropy_other, color='green', linestyle='--', \n                label=f'Cyrillic/Other Threshold ({max_entropy_other})')\n    plt.axvline(x=max_entropy_chinese, color='purple', linestyle='--', \n                label=f'Chinese Threshold ({max_entropy_chinese})')\n    \n    # Customize plot\n    plt.title('Distribution of Review Entropies')\n    plt.xlabel('Entropy')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('review_entropy_dist.png')\n    plt.show()\n    \n    # Optional: Separate by language\n    plt.figure(figsize=(12, 6))\n    sns.histplot(df[df['is_chinese']]['entropy'], bins=30, kde=True, color='purple', label='Chinese')\n    sns.histplot(df[df['is_cyrillic']]['entropy'], bins=30, kde=True, color='green', label='Cyrillic')\n    sns.histplot(df[~df['is_chinese'] & ~df['is_cyrillic']]['entropy'], bins=30, \n                 color='blue', label='English/Other')\n    plt.title('Entropy Distribution by Language')\n    plt.xlabel('Entropy')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig('review_entropy_lang.png')\n    plt.show()\n\n\nplot_entropy_distribution(merged_df.copy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_meaningful_review_multilingual(text, min_alpha_ratio=0.2, \n                                    max_entropy_english=4.5, \n                                    max_entropy_chinese=7.5, \n                                    max_entropy_other=5.5):\n    # Get rid of empty reviews\n    if not text or pd.isna(text):\n        return False\n    \n    text = str(text).lower()\n\n    # Count Unicode letters for alpha ratio\n    alpha_chars = sum(unicodedata.category(c).startswith('L') for c in text)\n    total_chars = len(text.replace(\" \", \"\"))\n    \n    if total_chars == 0 or alpha_chars / total_chars < min_alpha_ratio:\n        return False\n        \n    entropy = calculate_entropy(text)\n    \n    # Chinese reviews\n    if has_chinese(text):\n        return total_chars >= 5 and entropy <= max_entropy_chinese\n    \n    # Cyrillic (e.g., Russian) reviews\n    if has_cyrillic(text):\n        return total_chars >= 5 and entropy <= max_entropy_other\n    \n    # English or mixed reviews\n    if entropy > max_entropy_english:\n        return False\n    \n    # Make sure entropy is smaller than max_entropy param\n    entropy = calculate_entropy(text)\n    if entropy > max_entropy_english:  # High entropy indicates randomness\n        return 0\n    \n    # Make sure there is at least one real word in each entry (this will take a long time)\n    #word_list = set(words.words())  # NLTK's English word list\n    #tokens = re.findall(r'\\b[a-z]+\\b', text)  # return list of real words in review entry\n    #if not tokens:  # No words found\n    #    return 0\n    \n    #dict_words = sum(1 for token in tokens if token in word_list)\n    #dict_ratio = dict_words / len(tokens)\n    #if dict_ratio < min_dict_ratio:\n    #    return 0\n    \n    return 1\n\ndef flag_meaningless_reviews(test_df, review_col='reviews'):\n    \"\"\"Apply the meaningfulness check to a dataframe column and add a flag.\"\"\"\n    df = test_df.copy()\n    df['is_meaningful'] = df[review_col].apply(is_meaningful_review)\n    return df\n\n# DF with column for meaningless/meaningful tag\nresult_df = flag_meaningless_reviews(merged_df.copy())\n\n# DF filtered for only tag 1\nmeaningful_df = result_df[result_df['is_meaningful'] == 1].copy()\n\n# Summary of meaningful vs. meaningless\nprint(\"\\nSummary:\")\nprint(result_df['is_meaningful'].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What do some of the meaningless phrases look like? We see below that the alphabetic ratio and entropy are hyperparameters that need tuning. Some legitimate cases of \"meaningless\" are:\n1. The entry \"NaN'\n2. A single punctuation mark or typed out emoji\n\n**NEED TO ADDRESS**\n- Right now, it's detecting chinese characters as \"meaningless\" so wee nee to address this.","metadata":{}},{"cell_type":"code","source":"print(result_df[result_df['is_meaningful'] == False].sample(5).reviews.values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is an example of a substantive review written in chinese characters that has a higher entropy than it would in english.","metadata":{}},{"cell_type":"code","source":"chinese_string = '这是第一门自己申请助学金的课程，而霍普金斯大学也是我非常敬仰的学校，然而这门课程却不如我预想中的完美，第一是视频上来看，图形资源比较匮乏，多媒体资源没有很好的和讲课内容匹配，比如课程音频说的here 在屏幕上并没有显示出来具体的指示标识；第二是相关的资源太少了，如果相比莱顿大学的腹部解剖那门课程来说的确是比较枯燥。不过因为自己本身是这个专业的所以看起来障碍不是太大，不过对于没有肿瘤学基础的人来说上面的问题可能影响会比较大一些，希望团队能在这一方面继续改进。最后还是要感谢各位老师带来这么一门优秀的课程，我从中学到了很多！再次感谢'\nalphs = sum(unicodedata.category(c).startswith('L') for c in chinese_string)\ntotal = len(chinese_string.replace(\" \", \"\"))\nentropy = calculate_entropy(chinese_string)\nprint(f'Sum of chars = {alphs}')\nprint(f\"Total Chars = {total}\")\nprint(f\"Ratio = {alphs/total}\")\nprint(f\"Entropy = {entropy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Below is the english translation of the above chinese text\ntranslated_string = \"This is the first course for which I applied for a scholarship myself, and Johns Hopkins University is a school that I admire very much. However, this course is not as perfect as I expected. First, judging from the video, the graphic resources are relatively scarce, and the multimedia resources do not match the lecture content very well. For example, the course audio says here but there is no specific indicator on the screen; second, there are too few related resources. Compared with the abdominal anatomy course at Leiden University, it is indeed quite boring. However, since I am a major in this field, the obstacles do not seem to be too big. However, for those who do not have a foundation in oncology, the above problems may have a greater impact. I hope the team can continue to improve in this regard. Finally, I would like to thank all the teachers for bringing such an excellent course. I have learned a lot from it! Thanks again\"\nalphs = sum(unicodedata.category(c).startswith('L') for c in translated_string)\ntotal = len(translated_string.replace(\" \", \"\"))\nentropy = calculate_entropy(translated_string)\nprint(f'Sum of chars = {alphs}')\nprint(f\"Total Chars = {total}\")\nprint(f\"Ratio = {alphs/total}\")\nprint(f\"Entropy = {entropy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nresult_df[result_df['is_meaningful'] == 0].iloc[67477].reviews","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Distribution of Reviews by Language\n- Here we check to see how many reviews are in different languages. Let's investigate if there is a statistically significant difference in average entropy between different langugages' reviews\n- **NOTE:** Tagging the language of the DF takes ~2 hours on the Kaggle CPU","metadata":{}},{"cell_type":"code","source":"from langdetect import detect\nimport warnings\n\n# Suppress langdetect warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef detect_language(text):\n\n    if not isinstance(text, str) or pd.isna(text) or len(text.strip()) < 3:\n        return 'unknown'  # For NaN, empty, or very short text\n    try:\n        return detect(text)\n    except:\n        return 'unknown'  # Fallback for any detection errors\n\n# Get distribution of langugages using langdetect\ndef plot_language_distribution(df, review_col='reviews'):\n\n    # Detect language for each review\n    tqdm.pandas(desc=\"Detecting Languages\") \n    df['language'] = df[review_col].progress_apply(detect_language)\n    # Save this df with tagged languages to file\n    df.to_pickle('merged_df_w_langs.pkl')\n    # Count reviews per language\n    language_counts = df['language'].value_counts().reset_index()\n    language_counts.columns = ['language', 'count']\n    \n    # Print stats\n    print(\"Language Distribution:\")\n    print(language_counts.to_markdown())\n    \n    # Plot bar chart\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data=language_counts, x='language', y='count')\n    plt.title('Number of Reviews by Language')\n    plt.xlabel('Language (ISO 639-1 Code)')\n    plt.ylabel('Number of Reviews')\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(True, alpha=0.3)\n    \n    # Add count labels on top of bars\n    for i, row in language_counts.iterrows():\n        plt.text(i, row['count'], row['count'], ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('num_reviews_language.png')\n    plt.show()\n    return df # Return back the original df WITH a new language column\n\n# Install langdetect if needed: pip install langdetect\nlang_df = plot_language_distribution(merged_df.copy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:15:29.447943Z","iopub.execute_input":"2025-03-09T18:15:29.448361Z"}},"outputs":[{"name":"stderr","text":"Detecting Languages:   2%|▏         | 31524/1454711 [02:23<1:31:55, 258.04it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# FUnction to statistically detect differences in entropy of languages\ndef test_entropy_differences(df, review_col='review_comment'):\n    \"\"\"Test for significant differences in entropy across languages.\n       Feed this a df with entropies calculated/languages tagged for faster runtime\"\"\"\n    # Check if entropy/language tags are there\n    if 'entropy' not in df.columns:\n        df['entropy'] = df[review_col].apply(calculate_entropy)\n    if 'language' not in df.columns:\n        df['language'] = df[review_col].apply(detect_language)\n    \n    # Filter out 'unknown' and languages with too few samples (< 2)\n    language_counts = df['language'].value_counts()\n    valid_languages = language_counts[language_counts >= 2].index\n    df_filtered = df[df['language'].isin(valid_languages)].copy()\n    \n    if len(valid_languages) < 2:\n        print(\"Not enough languages with sufficient samples (>= 2 reviews) for comparison.\")\n        return\n    \n    # Group entropies by language\n    grouped_entropies = [df_filtered[df_filtered['language'] == lang]['entropy'].values \n                         for lang in valid_languages]\n\n        md_table = \"| Review Text | Word Count |\\n\"\n        md_table += \"|-------------|------------|\\n\"\n        for _, row in outlier_sample.iterrows():\n            # Truncate long text for display (optional)\n            text = row[review_col] if len(row[review_col]) < 50 else row[review_col][:47] + \"...\"\n            md_table += f\"| {text} | {row['word_count']} |\\n\"\n    \n    # Print summary stats in markdown table\n    print(\"Entropy Means by Language:\")\n    entropy_table = '| Language | Mean | Std Dev | N |\\n'\n    entropy_table += '|---|---|---|---|\\n'\n    for lang, group in df_filtered.groupby('language'):\n        entropy_table += f\"| {lang} | Mean = {group['entropy'].mean():.2f} | Std = {group['entropy'].std():.2f}| N = {len(group)}|\\n\" \n    print(entropy_table)\n\n    # Check normality (Shapiro-Wilk test) and print in markdown table\n    print(\"\\nNormality Test (Shapiro-Wilk):\")\n    normality_table = '| Language | p-value |\\n'\n    normality_table += '|--|--|\\n'\n    for lang, entropy_group in zip(valid_languages, grouped_entropies):\n        if len(entropy_group) > 3:  # Shapiro needs > 3 samples\n            stat, p = stats.shapiro(entropy_group)\n            normality_table += f\"|{lang} | p-value = {p:.4f} {'(not normal)' if p < 0.05 else '(normal)'}|\\n\"\n    print(normality_table)\n    \n    # Check equal variances (Levene's test)\n    stat, p = stats.levene(*grouped_entropies)\n    print(f\"\\nLevene's Test for Equal Variances: p-value = {p:.4f} {'(unequal)' if p < 0.05 else '(equal)'}\")\n    \n    # ANOVA (if assumptions hold)\n    if all(stats.shapiro(group)[1] >= 0.05 for group in grouped_entropies if len(group) > 3) and p >= 0.05:\n        f_stat, p_anova = stats.f_oneway(*grouped_entropies)\n        print(f\"\\nANOVA: F = {f_stat:.2f}, p-value = {p_anova:.4f}\")\n        if p_anova < 0.05:\n            print(\"Significant difference detected (ANOVA).\")\n            # Post-hoc: Tukey's HSD\n            tukey = pairwise_tukeyhsd(endog=df_filtered['entropy'], \n                                    groups=df_filtered['language'], \n                                    alpha=0.05)\n            print(tukey)\n    else:\n        # Kruskal-Wallis (non-parametric)\n        h_stat, p_kruskal = stats.kruskal(*grouped_entropies)\n        print(f\"\\nKruskal-Wallis: H = {h_stat:.2f}, p-value = {p_kruskal:.4f}\")\n        if p_kruskal < 0.05:\n            print(\"Significant difference detected (Kruskal-Wallis).\")\n            # Post-hoc: Pairwise Mann-Whitney U (optional)\n            from itertools import combinations\n            print(\"\\nPairwise Mann-Whitney U Tests (Bonferroni corrected):\")\n            pairs = list(combinations(valid_languages, 2))\n            alpha_corrected = 0.05 / len(pairs)\n            for lang1, lang2 in pairs:\n                u_stat, p = stats.mannwhitneyu(\n                    df_filtered[df_filtered['language'] == lang1]['entropy'],\n                    df_filtered[df_filtered['language'] == lang2]['entropy'],\n                    alternative='two-sided'\n                )\n                print(f\"{lang1} vs {lang2}: p-value = {p:.4f} {'(significant)' if p < alpha_corrected else ''}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Reviews per Course\n- Are there courses with very few or a ton of course reviews?","metadata":{}},{"cell_type":"code","source":"# Number of reviews per course\nreviews_per_course = df.groupby('course_id').size()\nprint(\"Reviews per Course Stats:\")\nprint(f'| Stat | Value |')\nprint(reviews_per_course.describe().to_markdown())\nprint(f'| median | {reviews_per_course.median()} |')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reviews per Course Stats:\n| Stat | Value |\n|:------|---------:|\n| count |   604    |\n| mean  |  2408.21 |\n| std   |  4599.04 |\n| min   |     3    |\n| 25%   |   374.25 |\n| 50%   |  1071.5  |\n| 75%   |  2408.25 |\n| max   | 45218    |\n| median | 1071.5 |","metadata":{}},{"cell_type":"code","source":"# Rebin the course review data to 1-10, 10-100, 100+\nplt.figure(figsize=(10, 6))\nsns.histplot(reviews_per_course, bins=10)\nplt.title('Histogram of Reviews per Course')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Log(Frequency)')\nplt.grid(True, alpha=0.3)\n#plt.xscale('log')\n#plt.yscale('log')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Not super meaningful. Let's do the same thing with review count as we did with character count","metadata":{}},{"cell_type":"code","source":"def plot_review_count_distribution(original_df, id_col='course_id', percentile_cutoff=95, sample_size=5):\n\n    copy_df = original_df.copy()\n    df = copy_df.groupby(id_col).size().reset_index(name='review_count')\n    \n    # Set review count cutoff to a given percentile for plotting. Defult to 95\n    cutoff = np.percentile(df['review_count'], percentile_cutoff)\n    print(f\"\\n{percentile_cutoff}th Percentile Cutoff: {cutoff:.0f} reviews\")\n\n    main_range = df['review_count'][df['review_count'] <= cutoff]\n    outliers = df[df['review_count'] > cutoff]\n    \n    # Plot histogram for main range\n    plt.figure(figsize=(10, 6))\n    sns.histplot(main_range, bins=30)\n    plt.title(f'Distribution of Course Review Counts (Up to {cutoff:.0f} Reviews)')\n    plt.xlabel('Number of Reviews')\n    plt.ylabel('Frequency')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('review_count_hist.png')\n    plt.show()\n\n    # Make output df of outliers and print to markdown\n    if not outliers.empty:\n        \n        outlier_sample = outliers[[id_col, 'review_count']].head(sample_size)\n        \n        md_table = \"| Course ID | Review Count |\\n\"\n        md_table += \"|-------------|------------|\\n\"\n        for _, row in outlier_sample.iterrows():\n            # Truncate long text for display (optional)\n            text = row[id_col]\n            md_table += f\"| {text} | {row['review_count']} |\\n\"\n        \n        print(\"\\nSample of Outlier Reviews:\")\n        print(md_table)\n\n\nplot_review_count_distribution(df, percentile_cutoff=97.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sample of Outlier Reviews:\n| Course ID | Review Count |\n|-------------|------------|\n| ai-for-everyone | 21624 |\n| data-scientists-tools | 17595 |\n| deep-neural-network | 17850 |\n| excel-essentials | 21248 |\n| gcp-fundamentals | 17419 |","metadata":{}},{"cell_type":"markdown","source":"#### Finding *Meaningful* Reviews\n- We'll use word/chracter counts and distributions to calculate entropy to attempt to tag meaningless entries","metadata":{}},{"cell_type":"markdown","source":"## Sentiment Analysis - DistilBERT Fine Tuning","metadata":{}},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}