{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":856783,"sourceType":"datasetVersion","datasetId":271075}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gibberish Detector\n## \"Gibberish\" vs. \"Substantive\"\n- **Gibberish:** Random strings (e.g., \"asdfjkl\"), typos (\"jhkd\"), or meaningless repetition (\"aaa\").\n- **Substantive:** Coherent sentences with intent (e.g., \"Great course, learned a lot\" or \"这是一个很好的课程\").\n- **Edge Cases:** Short but valid reviews (e.g., \"Good\"), multilingual mixes, or sarcasm.","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers huggingface_hub langdetect pycountry","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:52:09.992654Z","iopub.execute_input":"2025-03-15T16:52:09.993053Z","iopub.status.idle":"2025-03-15T16:52:20.682351Z","shell.execute_reply.started":"2025-03-15T16:52:09.993023Z","shell.execute_reply":"2025-03-15T16:52:20.680704Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport scipy.stats as stats\nimport re\nimport nltk\nfrom nltk.corpus import words\nfrom collections import Counter\nimport math\nimport unicodedata\nimport time\nfrom tqdm import tqdm\nimport pickle\nfrom langdetect import detect\nimport warnings\nimport pycountry\nfrom scipy import stats\nfrom collections import defaultdict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:11.295622Z","iopub.execute_input":"2025-03-15T17:39:11.296050Z","iopub.status.idle":"2025-03-15T17:39:11.302237Z","shell.execute_reply.started":"2025-03-15T17:39:11.296013Z","shell.execute_reply":"2025-03-15T17:39:11.300862Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"---------------------BEGIN DATA IMPORT------------","metadata":{}},{"cell_type":"markdown","source":"### Importing Data\n- The original data used an older character encoding\n- We'll save them as utf-8 so we can add the csv's to our project dataset","metadata":{}},{"cell_type":"code","source":"# Try different encodings\nfile_path = '/kaggle/input/gibberish-text-classification/Amazon.csv'  # Replace with your file path\ntry:\n    df = pd.read_csv(file_path, encoding='utf-8')  # Default, might fail\nexcept UnicodeDecodeError:\n    print(\"UTF-8 failed, trying other encodings...\")\n    try:\n        df = pd.read_csv(file_path, encoding='windows-1252')\n        print(\"Loaded with Windows-1252\")\n    except UnicodeDecodeError:\n        try:\n            df = pd.read_csv(file_path, encoding='iso-8859-1')\n            print(\"Loaded with ISO-8859-1\")\n        except UnicodeDecodeError:\n            try:\n                df = pd.read_csv(file_path, encoding='utf-16')\n                print(\"Loaded with UTF-16\")\n            except UnicodeDecodeError:\n                print(\"All common encodings failed. Check file encoding or corruption.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T23:16:43.643716Z","iopub.execute_input":"2025-03-13T23:16:43.644344Z","iopub.status.idle":"2025-03-13T23:17:18.296890Z","shell.execute_reply.started":"2025-03-13T23:16:43.644286Z","shell.execute_reply":"2025-03-13T23:17:18.295623Z"}},"outputs":[{"name":"stdout","text":"UTF-8 failed, trying other encodings...\nLoaded with Windows-1252\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Load with Windows-1252, save as UTF-8 for adding to our custom kaggle dataset\ndef convert_encoding_type(file, out):\n    # Convert data to more universal encoding type\n    df = pd.read_csv(file, encoding='windows-1252')\n    df.to_csv(out, encoding='utf-8', index=False)\n    print(f\"Converted and saved as {out}\")\n    return\n\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Amazon.csv', 'amazon_reviews.csv')\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Gibberish.csv', 'amazon_gibberish.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T23:42:25.717617Z","iopub.execute_input":"2025-03-13T23:42:25.718054Z","iopub.status.idle":"2025-03-13T23:42:44.222935Z","shell.execute_reply.started":"2025-03-13T23:42:25.718018Z","shell.execute_reply":"2025-03-13T23:42:44.221855Z"}},"outputs":[{"name":"stdout","text":"Converted and saved as amazon_reviews.csv\nConverted and saved as amazon_gibberish.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"-----------------------------END DATA CREATION---------------------","metadata":{}},{"cell_type":"markdown","source":"## EDA and Feature Extraction","metadata":{}},{"cell_type":"code","source":"reviews_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Amazon.csv', encoding='windows-1252', header=None, names=['rating', 'review'])\ngibb_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Gibberish.csv', encoding='windows-1252')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:44.432934Z","iopub.execute_input":"2025-03-15T17:39:44.433308Z","iopub.status.idle":"2025-03-15T17:39:56.912245Z","shell.execute_reply.started":"2025-03-15T17:39:44.433281Z","shell.execute_reply":"2025-03-15T17:39:56.911347Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"According to the original data set at https://www.kaggle.com/datasets/bittlingmayer/amazonreviews, __label__2 means 4 and 5 star product reviews and label 1 is 1 and 2 start reviews. Looks like there's roughly equal amounts of both (positive and negative sentiment)","metadata":{}},{"cell_type":"code","source":"reviews_df.rating.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:56.913297Z","iopub.execute_input":"2025-03-15T17:39:56.913564Z","iopub.status.idle":"2025-03-15T17:39:57.011836Z","shell.execute_reply.started":"2025-03-15T17:39:56.913542Z","shell.execute_reply":"2025-03-15T17:39:57.010648Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"rating\n__label__2    530161\n__label__1    518415\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.013559Z","iopub.execute_input":"2025-03-15T17:39:57.013910Z","iopub.status.idle":"2025-03-15T17:39:57.049830Z","shell.execute_reply.started":"2025-03-15T17:39:57.013882Z","shell.execute_reply":"2025-03-15T17:39:57.048765Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"       rating                                             review\n0  __label__2  Stuning even for the non-gamer: This sound tra...\n1  __label__2  The best soundtrack ever to anything.: I'm rea...\n2  __label__2  Amazing!: This soundtrack is my favorite music...\n3  __label__2  Excellent Soundtrack: I truly like this soundt...\n4  __label__2  Remember, Pull Your Jaw Off The Floor After He...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>__label__2</td>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>__label__2</td>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>__label__2</td>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>__label__2</td>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>__label__2</td>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Create sentiment column with 1 as positive (4/5 star), 0 negative (1,2 star)\nreviews_df['sentiment'] = (reviews_df['rating'] == '__label__2')*1\nreviews_df['gibberish'] = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.051341Z","iopub.execute_input":"2025-03-15T17:39:57.051803Z","iopub.status.idle":"2025-03-15T17:39:57.158871Z","shell.execute_reply.started":"2025-03-15T17:39:57.051763Z","shell.execute_reply":"2025-03-15T17:39:57.157787Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"There is a massive class imbalance with over 1 million real reviews and only 3767 gibberish reviews. We'll need to do some resampling.","metadata":{}},{"cell_type":"code","source":"gibb_df.rename(columns={'Response': 'review', 'Label': 'gibberish'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.160072Z","iopub.execute_input":"2025-03-15T17:39:57.160454Z","iopub.status.idle":"2025-03-15T17:39:57.168249Z","shell.execute_reply.started":"2025-03-15T17:39:57.160415Z","shell.execute_reply":"2025-03-15T17:39:57.167288Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Make new merged dataframe of just the reviews and whether they are gibberish\nmerged_df = pd.concat([gibb_df, reviews_df[['review', 'gibberish']]], ignore_index=True)\nmerged_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.169209Z","iopub.execute_input":"2025-03-15T17:39:57.169551Z","iopub.status.idle":"2025-03-15T17:39:57.283780Z","shell.execute_reply.started":"2025-03-15T17:39:57.169518Z","shell.execute_reply":"2025-03-15T17:39:57.282674Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                    review  gibberish\n0                                                      ggg          1\n1                                         hgghghghghghghhg          1\n2                              ufdhgjndfnvbhfdjvnjkmfgbdfg          1\n3                                                  dbdbdbd          1\n4                                                  dfgdfgd          1\n...                                                    ...        ...\n1052338  Cheap and flimsy: This was bought for an event...          0\n1052339  Total waste of money: This was a total waste o...          0\n1052340  Whitmor budget garment rack: I purchased the W...          0\n1052341  Serves its purpose: I bought this to put in my...          0\n1052342  Conversation on conversation: The so-called \"r...          0\n\n[1052343 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>gibberish</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ggg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hgghghghghghghhg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ufdhgjndfnvbhfdjvnjkmfgbdfg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dbdbdbd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dfgdfgd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052338</th>\n      <td>Cheap and flimsy: This was bought for an event...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052339</th>\n      <td>Total waste of money: This was a total waste o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052340</th>\n      <td>Whitmor budget garment rack: I purchased the W...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052341</th>\n      <td>Serves its purpose: I bought this to put in my...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052342</th>\n      <td>Conversation on conversation: The so-called \"r...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1052343 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"len(merged_df) - len(gibb_df) - len(reviews_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.284799Z","iopub.execute_input":"2025-03-15T17:39:57.285217Z","iopub.status.idle":"2025-03-15T17:39:57.291607Z","shell.execute_reply.started":"2025-03-15T17:39:57.285179Z","shell.execute_reply":"2025-03-15T17:39:57.290623Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# FEATURE - Calculate entropy\ndef calculate_entropy(text):\n    \"\"\"Calculate Shannon entropy of the text to detect randomness.\"\"\"\n    if not text:\n        return 0\n    if not isinstance(text, str) or pd.isna(text):\n        return 0  # Return 0 for NaN or non-string values\n    text = str(text).lower()\n    length = len(text)\n    if length == 0:  # Handle empty strings\n        return 0\n    char_counts = Counter(text)\n    entropy = -sum((count/length) * math.log2(count/length) for count in char_counts.values())\n    return entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:39:57.293545Z","iopub.execute_input":"2025-03-15T17:39:57.293919Z","iopub.status.idle":"2025-03-15T17:39:57.314682Z","shell.execute_reply.started":"2025-03-15T17:39:57.293881Z","shell.execute_reply":"2025-03-15T17:39:57.313320Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# FEATURE - Can detect language\nfrom langdetect import detect\nimport warnings\n\n# Suppress langdetect warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef detect_language(text):\n\n    if not isinstance(text, str) or pd.isna(text) or len(text.strip()) < 3:\n        return 'unknown'  # For NaN, empty, or very short text\n    try:\n        return detect(text)\n    except:\n        return 'unknown'  # Fallback for any detection errors\n\n# Returns 0 if we can't find the langauge, 1 if we can\ndef cannot_detect_language(text):\n    if text == 'unknown':\n        return 1\n    else:\n        return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next feature we'll look at is a flag of type of a few main alphabets. In our EDA we showed that outside of English, the most popular of our Coursera review languages mostly used a Latin or very close to Latin alphabet (somali, afrikaans, and tagalog for example use some variation of latin). We'll make flags for \n1. chinese\n2. cyrillic (for russian, slovenian)\n3. hangul (for korean)\n4. Latin or latin variant","metadata":{}},{"cell_type":"code","source":"# FEATURE - Type of Alphabet (run time 3 minutes)\ndef detect_alphabets(text):\n\n    if not isinstance(text, str) or not text:\n        return {\n            'Chinese': {'present': False, 'count': 0},\n            'Cyrillic': {'present': False, 'count': 0},\n            'Hangul': {'present': False, 'count': 0},\n            'Latin': {'present': False, 'count': 0}\n        }\n    \n    # Define Unicode ranges\n    ranges = {\n        'Chinese': (0x4E00, 0x9FFF),         # CJK Unified Ideographs\n        'Cyrillic': (0x0400, 0x04FF),       # Basic Cyrillic\n        'Hangul': (0xAC00, 0xD7AF),         # Hangul Syllables\n        'Latin': [(0x0000, 0x007F),         # Basic Latin\n                  (0x00A0, 0x00FF),         # Latin-1 Supplement\n                  (0x0100, 0x017F)],        # Latin Extended-A\n    }\n    \n    # Count characters per alphabet\n    alphabet_counts = defaultdict(int)\n    for char in text:\n        char_code = ord(char)\n        \n        # Check Chinese\n        if ranges['Chinese'][0] <= char_code <= ranges['Chinese'][1]:\n            alphabet_counts['Chinese'] += 1\n        \n        # Check Cyrillic\n        if ranges['Cyrillic'][0] <= char_code <= ranges['Cyrillic'][1]:\n            alphabet_counts['Cyrillic'] += 1\n        \n        # Check Hangul\n        if ranges['Hangul'][0] <= char_code <= ranges['Hangul'][1]:\n            alphabet_counts['Hangul'] += 1\n        \n        # Check Latin (multiple ranges)\n        for start, end in ranges['Latin']:\n            if start <= char_code <= end:\n                alphabet_counts['Latin'] += 1\n                break  # Stop after first match\n    \n    # Build result dictionary\n    result = {\n        'Chinese': {'present': alphabet_counts['Chinese'] > 0, 'count': alphabet_counts['Chinese']},\n        'Cyrillic': {'present': alphabet_counts['Cyrillic'] > 0, 'count': alphabet_counts['Cyrillic']},\n        'Abakada': {'present': alphabet_counts['Abakada'] > 0, 'count': alphabet_counts['Abakada']},\n        'Hangul': {'present': alphabet_counts['Hangul'] > 0, 'count': alphabet_counts['Hangul']},\n        'Latin': {'present': alphabet_counts['Latin'] > 0, 'count': alphabet_counts['Latin']}\n    }\n    \n    return result\n\ndef create_alphabet_tag_feature(df, review_col='review'):\n    # Create alphabets column with dictionary structure above\n    tqdm.pandas()\n    df['alphabets'] = df[review_col].progress_apply(detect_alphabets)\n\n    # Create one-hot tag of different alphabets\n    df['has_chinese'] = df['alphabets'].apply(lambda x: x['Chinese']['present'])\n    df['has_cyrillic'] = df['alphabets'].apply(lambda x: x['Cyrillic']['present'])\n    df['has_abakada'] = df['alphabets'].apply(lambda x: x['Abakada']['present'])\n    df['has_hangul'] = df['alphabets'].apply(lambda x: x['Hangul']['present'])\n    df['has_latin'] = df['alphabets'].apply(lambda x: x['Latin']['present'])\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:42:52.350431Z","iopub.execute_input":"2025-03-15T17:42:52.350870Z","iopub.status.idle":"2025-03-15T17:42:52.362773Z","shell.execute_reply.started":"2025-03-15T17:42:52.350827Z","shell.execute_reply":"2025-03-15T17:42:52.361203Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 3x FEATURES - Total characters in review, word count of review, avg word length\ndef word_count(text):\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    return word_count\n\ndef char_count(text):\n    return len(text)\n\ndef create_word_and_char_counts_feature(df, review_col='review'):\n    tqdm.pandas()\n    df['word_count'] = df[review_col].progress_apply(word_count)\n    df['n_chars'] = df[review_col].progress_apply(char_count)\n    return df\n\ndef get_avg_word_length(text):\n    # avg word length\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    avg_word_length = sum(len(word) for word in words if word) / max(1, word_count) if words else 0\n    return avg_word_length\n\ndef create_avg_word_length_feature(df, review_col='review'):\n    tqdm.pandas()\n    df['avg_word_length'] = df[review_col].progress_apply(get_avg_word_length)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Amount of Reptition\ndef get_max_repeated(text):\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    return max_repeates\ndef create_repetition_feature(df, review_col='review'):\n    tqdm.pandas()\n    df['max_repeated'] = df[review_col].progress_apply(get_max_repeated)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Punctuation Ratio\ndef get_punct_ratio(text):\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    return punct_ratio\ndef create_punct_ratio_feature(df, review_col='review'):\n    df['punct_ratio'] = df[review_col].progress_apply(get_punct_ratio)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Contains common n-grams\n# Step 1 - Build n-gram reference from sample of real reviews\ndef build_ngram_reference(texts, n=2, top_k=1000, sample_size=10000):\n    \"\"\"\n    Build a set of common n-grams from a list of texts, assuming most are meaningful.\n    WARNING - Must only be done on training set\n    \n    Parameters:\n    - texts: List of text strings\n    - n: N-gram size\n    - top_k: Number of top n-grams to keep\n    - sample_size: Number of texts to sample (to speed up)\n    \n    Returns:\n    - Set of common n-grams\n    \"\"\"\n    # Sample texts to avoid over-processing (e.g., 1.19M reviews)\n    if len(texts) > sample_size:\n        texts = np.random.choice(texts, sample_size, replace=False)\n    \n    # Generate n-grams\n    ngrams = Counter()\n    for text in tqdm(texts, desc=\"Building n-gram reference\"):\n        text = str(text).lower()\n        for i in range(len(text) - n + 1):\n            ngram = text[i:i+n]\n            if not ngram.isspace():\n                ngrams[ngram] += 1\n    \n    # Return top k most common n-grams\n    return set([ngram for ngram, _ in ngrams.most_common(top_k)])\n\ndef create_ngram_coherence_feature(df, ngram_ref, review_col='review'):\n    # N-gram coherence\n    # Get reference set of ngrams\n    text_lower = text.lower()\n    total_ngrams = max(1, len(text_lower) - n + 1)\n    valid_ngrams = sum(1 for i in range(total_ngrams) if text_lower[i:i+n] in ngram_ref)\n    ngram_coherence = valid_ngrams / total_ngrams\n    return ngram_coherence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tagged_df = create_alphabet_tag_feature(merged_df.copy())\n# create ngram_ref from training data only\nngram_ref = build_ngram_reference(df[review_col].dropna().tolist(), \n                                      n=2, top_k=1000, sample_size=10000)\ntagged_df.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FEATURE - ","metadata":{}},{"cell_type":"code","source":"# ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def language_id","metadata":{}},{"cell_type":"markdown","source":"Existing Features\nEntropy:\nCalculate Shannon entropy of characters (as before).\n\nGibberish tends to have higher or flatter entropy; substantive reviews have structured patterns.\n\nCode: Already in your toolkit (e.g., calculate_entropy).\n\nLanguage Detection:\nUse langdetect to get the language code.\n\nFeature: Binary flag (1 if \"unknown\" or low-confidence language, 0 if recognized).\n\nCode: detect_language from earlier.\n\nWord Frequency:\nCount known common words per language (e.g., \"good,\" \"bad\" in English; \"好\" in Chinese).\n\nFeature: Ratio of common words to total words.\n\nRequires a dictionary per language (e.g., from NLTK or custom lists).\n\nNew Features\nWord Count:\nNumber of words (split by spaces or language-specific tokenizers).\n\nGibberish often has fewer \"words\" or unrecognizable tokens.\n\nFeature: Integer count or log(count).\n\nCharacter Length:\nTotal characters in the review.\n\nVery short reviews (e.g., \"a\") or overly long random strings may indicate gibberish.\n\nFeature: Integer or normalized (length / max_length).\n\nRepetition:\nMeasure character or word repetition (e.g., \"aaa\" or \"good good good\").\n\nFeature: Max consecutive repeats or entropy of word frequency.\n\nCode: Use collections.Counter on words/characters.\n\nPunctuation Ratio:\nProportion of punctuation to alphanumeric characters.\n\nGibberish might over- or under-use punctuation (e.g., \"!!!\").\n\nFeature: Float (punctuation_count / total_chars).\n\nN-Gram Coherence:\nCheck if bigrams/trigrams match common patterns in each language.\n\nGibberish lacks typical n-gram structure (e.g., \"th\" in English vs. \"xz\").\n\nFeature: Proportion of valid n-grams (requires language-specific n-gram lists).\n\nText Complexity:\nAverage word length or readability score (e.g., Flesch-Kincaid, simplified for multilingual).\n\nGibberish often has inconsistent or extreme word lengths.\n\nFeature: Float (mean word length).\n\nEmbedding-Based Features (Advanced):\nUse a pre-trained multilingual model (e.g., BERT, XLM-R) to get sentence embeddings.\n\nFeature: Cosine similarity to a centroid of substantive reviews or anomaly score.\n\nRequires more computation but leverages semantic coherence.\n\n","metadata":{}},{"cell_type":"code","source":"\n# Feature extraction function\ndef extract_features(text):\n    if not isinstance(text, str) or pd.isna(text):\n        text = \"\"\n    \n    # Entropy\n    length = len(text)\n    if length == 0:\n        entropy = 0\n    else:\n        char_counts = Counter(text.lower())\n        entropy = -sum((count/length) * np.log2(count/length) for count in char_counts.values())\n    \n    # Language\n    lang = detect(text) if length >= 3 else 'unknown'\n    is_unknown = 1 if lang == 'unknown' else 0\n    \n    # Word count and common words (example for English)\n    words = re.split(r'\\s+', text.strip())\n    word_count = len(words)\n    common_words = {'good', 'great', 'bad', 'course', 'learn'}  # Expand per language\n    common_word_ratio = sum(1 for w in words if w.lower() in common_words) / max(1, word_count)\n    \n    # Character length\n    char_length = len(text)\n    \n    # Repetition\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    \n    # Punctuation ratio\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    \n    return [entropy, is_unknown, word_count, common_word_ratio, char_length, max_repeats, punct_ratio]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom langdetect import detect\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport re\n\n# Feature extraction function\ndef extract_features(text):\n    if not isinstance(text, str) or pd.isna(text):\n        text = \"\"\n    \n    # Entropy\n    length = len(text)\n    if length == 0:\n        entropy = 0\n    else:\n        char_counts = Counter(text.lower())\n        entropy = -sum((count/length) * np.log2(count/length) for count in char_counts.values())\n    \n    # Language\n    lang = detect(text) if length >= 3 else 'unknown'\n    is_unknown = 1 if lang == 'unknown' else 0\n    \n    # Word count and common words (example for English)\n    words = re.split(r'\\s+', text.strip())\n    word_count = len(words)\n    common_words = {'good', 'great', 'bad', 'course', 'learn'}  # Expand per language\n    common_word_ratio = sum(1 for w in words if w.lower() in common_words) / max(1, word_count)\n    \n    # Character length\n    char_length = len(text)\n    \n    # Repetition\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    \n    # Punctuation ratio\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    \n    return [entropy, is_unknown, word_count, common_word_ratio, char_length, max_repeats, punct_ratio]\n\n# Load labeled product review dataset\nproduct_df = pd.read_csv('product_reviews_labeled.csv')  # Assume columns: 'review', '\n\n# Extract features\nX = np.array([extract_features(review) for review in product_df['review']])\ny = product_df['is_gibberish'].values  # 1 = gibberish, 0 = not gibberish\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Apply to course reviews (merged_df)\ncourse_features = np.array([extract_features(review) for review in merged_df['reviews']])\ncourse_predictions = clf.predict(course_features)\nmerged_df['is_gibberish'] = course_predictions\n\n# Save results\nmerged_df.to_parquet('course_reviews_classified.parquet')\nprint(\"Course reviews classified and saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}