{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":856783,"sourceType":"datasetVersion","datasetId":271075}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gibberish Detector\n## \"Gibberish\" vs. \"Substantive\"\n- **Gibberish:** Random strings (e.g., \"asdfjkl\"), typos (\"jhkd\"), or meaningless repetition (\"aaa\").\n- **Substantive:** Coherent sentences with intent (e.g., \"Great course, learned a lot\" or \"这是一个很好的课程\").\n- **Edge Cases:** Short but valid reviews (e.g., \"Good\"), multilingual mixes, or sarcasm.","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers huggingface_hub langdetect pycountry","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport scipy.stats as stats\nimport re\nimport nltk\nfrom nltk.corpus import words\nfrom collections import Counter\nimport math\nimport unicodedata\nimport time\nfrom tqdm import tqdm\nimport pickle\nfrom langdetect import detect\nimport warnings\nimport pycountry\nfrom scipy import stats\nfrom collections import defaultdict\nimport itertools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------------------BEGIN DATA IMPORT------------","metadata":{}},{"cell_type":"markdown","source":"### Importing Data\n- The original data used an older character encoding\n- We'll save them as utf-8 so we can add the csv's to our project dataset","metadata":{}},{"cell_type":"code","source":"# Try different encodings\nfile_path = '/kaggle/input/gibberish-text-classification/Amazon.csv'  # Replace with your file path\ntry:\n    df = pd.read_csv(file_path, encoding='utf-8')  # Default, might fail\nexcept UnicodeDecodeError:\n    print(\"UTF-8 failed, trying other encodings...\")\n    try:\n        df = pd.read_csv(file_path, encoding='windows-1252')\n        print(\"Loaded with Windows-1252\")\n    except UnicodeDecodeError:\n        try:\n            df = pd.read_csv(file_path, encoding='iso-8859-1')\n            print(\"Loaded with ISO-8859-1\")\n        except UnicodeDecodeError:\n            try:\n                df = pd.read_csv(file_path, encoding='utf-16')\n                print(\"Loaded with UTF-16\")\n            except UnicodeDecodeError:\n                print(\"All common encodings failed. Check file encoding or corruption.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load with Windows-1252, save as UTF-8 for adding to our custom kaggle dataset\ndef convert_encoding_type(file, out):\n    # Convert data to more universal encoding type\n    df = pd.read_csv(file, encoding='windows-1252')\n    df.to_csv(out, encoding='utf-8', index=False)\n    print(f\"Converted and saved as {out}\")\n    return\n\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Amazon.csv', 'amazon_reviews.csv')\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Gibberish.csv', 'amazon_gibberish.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------------END DATA CREATION---------------------","metadata":{}},{"cell_type":"markdown","source":"## EDA and Feature Extraction","metadata":{}},{"cell_type":"code","source":"reviews_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Amazon.csv', encoding='windows-1252', header=None, names=['rating', 'review'])\ngibb_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Gibberish.csv', encoding='windows-1252')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"According to the original data set at https://www.kaggle.com/datasets/bittlingmayer/amazonreviews, __label__2 means 4 and 5 star product reviews and label 1 is 1 and 2 start reviews. Looks like there's roughly equal amounts of both (positive and negative sentiment)","metadata":{}},{"cell_type":"code","source":"reviews_df.rating.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create sentiment column with 1 as positive (4/5 star), 0 negative (1,2 star)\nreviews_df['sentiment'] = (reviews_df['rating'] == '__label__2')*1\nreviews_df['is_gibberish'] = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is a massive class imbalance with over 1 million real reviews and only 3767 gibberish reviews. We'll need to do some resampling.","metadata":{}},{"cell_type":"code","source":"gibb_df.rename(columns={'Response': 'review', 'Label': 'is_gibberish'}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make new merged dataframe of just the reviews and whether they are gibberish\nmerged_df = pd.concat([gibb_df, reviews_df[['review', 'is_gibberish']]], ignore_index=True)\nmerged_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(merged_df) - len(gibb_df) - len(reviews_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Functions to Build Features","metadata":{}},{"cell_type":"code","source":"# FEATURE - Calculate entropy\ndef calculate_entropy(text):\n    \"\"\"Calculate Shannon entropy of the text to detect randomness.\"\"\"\n    if not text:\n        return 0\n    if not isinstance(text, str) or pd.isna(text):\n        return 0  # Return 0 for NaN or non-string values\n    text = str(text).lower()\n    length = len(text)\n    if length == 0:  # Handle empty strings\n        return 0\n    char_counts = Counter(text)\n    entropy = -sum((count/length) * math.log2(count/length) for count in char_counts.values())\n    return entropy\n\ndef create_entropy_feature(df, review_col='review'):\n    tqdm.pandas(desc='Calculating entropies: ')\n    df['entropy'] = df['review'].progress_apply(calculate_entropy)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Can detect language\nfrom langdetect import detect\nimport warnings\n\n# Suppress langdetect warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef detect_language(text):\n\n    if not isinstance(text, str) or pd.isna(text) or len(text.strip()) < 3:\n        return 'unknown'  # For NaN, empty, or very short text\n    try:\n        return detect(text)\n    except:\n        return 'unknown'  # Fallback for any detection errors\n\n# Returns 0 if we can't find the langauge, 1 if we can\ndef cannot_detect_language(text):\n    if text == 'unknown':\n        return 1\n    else:\n        return 0\n\ndef create_can_detect_feature(df, review_col='review'):\n    tqdm.pandas(desc=\"Detecting Language...\")\n    df['language'] = df[review_col].progress_apply(detect_language)\n    df['cannot_detect_language'] = df['language'].progress_apply(cannot_detect_language)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next feature we'll look at is a flag of type of a few main alphabets. In our EDA we showed that outside of English, the most popular of our Coursera review languages mostly used a Latin or very close to Latin alphabet (somali, afrikaans, and tagalog for example use some variation of latin). We'll make flags for \n1. chinese\n2. cyrillic (for russian, slovenian)\n3. hangul (for korean)\n4. Latin or latin variant","metadata":{}},{"cell_type":"code","source":"# FEATURE - Type of Alphabet (run time 3 minutes)\ndef detect_alphabets(text):\n\n    if not isinstance(text, str) or not text:\n        return {\n            'Chinese': {'present': False, 'count': 0},\n            'Cyrillic': {'present': False, 'count': 0},\n            'Hangul': {'present': False, 'count': 0},\n            'Latin': {'present': False, 'count': 0}\n        }\n    \n    # Define Unicode ranges\n    ranges = {\n        'Chinese': (0x4E00, 0x9FFF),         # CJK Unified Ideographs\n        'Cyrillic': (0x0400, 0x04FF),       # Basic Cyrillic\n        'Hangul': (0xAC00, 0xD7AF),         # Hangul Syllables\n        'Latin': [(0x0000, 0x007F),         # Basic Latin\n                  (0x00A0, 0x00FF),         # Latin-1 Supplement\n                  (0x0100, 0x017F)],        # Latin Extended-A\n    }\n    \n    # Count characters per alphabet\n    alphabet_counts = defaultdict(int)\n    for char in text:\n        char_code = ord(char)\n        \n        # Check Chinese\n        if ranges['Chinese'][0] <= char_code <= ranges['Chinese'][1]:\n            alphabet_counts['Chinese'] += 1\n        \n        # Check Cyrillic\n        if ranges['Cyrillic'][0] <= char_code <= ranges['Cyrillic'][1]:\n            alphabet_counts['Cyrillic'] += 1\n        \n        # Check Hangul\n        if ranges['Hangul'][0] <= char_code <= ranges['Hangul'][1]:\n            alphabet_counts['Hangul'] += 1\n        \n        # Check Latin (multiple ranges)\n        for start, end in ranges['Latin']:\n            if start <= char_code <= end:\n                alphabet_counts['Latin'] += 1\n                break  # Stop after first match\n    \n    # Build result dictionary\n    result = {\n        'Chinese': {'present': alphabet_counts['Chinese'] > 0, 'count': alphabet_counts['Chinese']},\n        'Cyrillic': {'present': alphabet_counts['Cyrillic'] > 0, 'count': alphabet_counts['Cyrillic']},\n        'Abakada': {'present': alphabet_counts['Abakada'] > 0, 'count': alphabet_counts['Abakada']},\n        'Hangul': {'present': alphabet_counts['Hangul'] > 0, 'count': alphabet_counts['Hangul']},\n        'Latin': {'present': alphabet_counts['Latin'] > 0, 'count': alphabet_counts['Latin']}\n    }\n    \n    return result\n\ndef create_alphabet_tag_feature(df, review_col='review'):\n    # Create alphabets column with dictionary structure above\n    tqdm.pandas(desc='Creating alphabet tagging feature...')\n    df['alphabets'] = df[review_col].progress_apply(detect_alphabets)\n\n    # Create one-hot tag of different alphabets\n    df['has_chinese'] = df['alphabets'].apply(lambda x: x['Chinese']['present'])\n    df['has_cyrillic'] = df['alphabets'].apply(lambda x: x['Cyrillic']['present'])\n    df['has_abakada'] = df['alphabets'].apply(lambda x: x['Abakada']['present'])\n    df['has_hangul'] = df['alphabets'].apply(lambda x: x['Hangul']['present'])\n    df['has_latin'] = df['alphabets'].apply(lambda x: x['Latin']['present'])\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3x FEATURES - Total characters in review, word count of review, avg word length\ndef word_count(text):\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    return word_count\n\ndef char_count(text):\n    return len(text)\n\ndef create_word_and_char_counts_feature(df, review_col='review'):\n    tqdm.pandas(desc='Getting word/char counts...')\n    df['word_count'] = df[review_col].progress_apply(word_count)\n    df['n_chars'] = df[review_col].progress_apply(char_count)\n    return df\n\ndef get_avg_word_length(text):\n    # avg word length\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    avg_word_length = sum(len(word) for word in words if word) / max(1, word_count) if words else 0\n    return avg_word_length\n\ndef create_avg_word_length_feature(df, review_col='review'):\n    tqdm.pandas(desc='Getting avg word length feature...')\n    df['avg_word_length'] = df[review_col].progress_apply(get_avg_word_length)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Amount of Reptition\ndef get_max_repeated(text):\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    return max_repeats\ndef create_repetition_feature(df, review_col='review'):\n    tqdm.pandas(desc='Creating repetition feature...')\n    df['max_repeated'] = df[review_col].progress_apply(get_max_repeated)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Punctuation Ratio\ndef get_punct_ratio(text):\n    char_length = len(text)\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    return punct_ratio\ndef create_punct_ratio_feature(df, review_col='review'):\n    tqdm.pandas(desc='Creating punctuation ratio feature...')\n    df['punct_ratio'] = df[review_col].progress_apply(get_punct_ratio)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - Contains common n-grams\n# Step 1 - Build n-gram reference from sample of real reviews\ndef build_ngram_reference(texts, n=2, top_k=1000, sample_size=10000):\n    \"\"\"\n    Build a set of common n-grams from a list of texts, assuming most are meaningful.\n    WARNING - Must only be done on training set\n    \n    Parameters:\n    - texts: List of text strings\n    - n: N-gram size\n    - top_k: Number of top n-grams to keep\n    - sample_size: Number of texts to sample (to speed up)\n    \n    Returns:\n    - Set of common n-grams\n    \"\"\"\n    # Sample texts to avoid over-processing (e.g., 1.19M reviews)\n    if len(texts) > sample_size:\n        texts = np.random.choice(texts, sample_size, replace=False)\n    \n    # Generate n-grams\n    ngrams = Counter()\n    for text in tqdm(texts, desc=\"Building n-gram reference...\"):\n        text = str(text).lower()\n        for i in range(len(text) - n + 1):\n            ngram = text[i:i+n]\n            if not ngram.isspace():\n                ngrams[ngram] += 1\n    \n    # Return top k most common n-grams\n    return set([ngram for ngram, _ in ngrams.most_common(top_k)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE - ngram coherence, fraction of ngrams that appear in list of common ngrams\ndef get_ngram_coherence(text, n=2):\n    text_lower = text.lower()\n    total_ngrams = max(1, len(text_lower) - n + 1)\n    valid_ngrams = sum(1 for i in range(total_ngrams) if text_lower[i:i+n] in ngram_ref)\n    ngram_coherence = valid_ngrams / total_ngrams\n    return ngram_coherence\n\ndef create_ngram_coherence_feature(df, ngram_ref, review_col='review'):\n    tqdm.pandas(desc='Calcualting ngram coherenece...')\n    df['ngram_coherence'] = df[review_col].progress_apply(get_ngram_coherence)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import XLMRobertaTokenizer, XLMRobertaModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained XLM-R model and tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaModel.from_pretrained('xlm-roberta-base')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()\n\n# Function to get embeddings in batches\ndef get_embeddings(texts, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n        batch_texts = texts[i:i + batch_size]\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        # Use [CLS] token embedding (first token)\n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.append(batch_embeddings)\n    return np.vstack(embeddings)\n\n# Compute centroid from substantive reviews\ndef compute_centroid(df, review_col='review', label_col='is_gibberish', sample_size=10000):\n    # Use real reviews (training data only)\n    real_texts = df[df[label_col] == 0][review_col].dropna().tolist()\n    if len(real_texts) > sample_size:\n        real_texts = np.random.choice(real_texts, sample_size, replace=False).tolist()\n    embeddings = get_embeddings(real_texts)\n    return np.mean(embeddings, axis=0)\n\n# Add embedding-based features\ndef add_embedding_features(df, centroid, review_col='review', embed_path=None):\n    if embed_path and os.path.exists(embed_path):\n        print(f\"Loading embeddings from {embed_path}\")\n        embeddings = np.load(embed_path)\n    else:\n        texts = df[review_col].fillna('').tolist()\n        embeddings = get_embeddings(texts)\n        if embed_path:\n            np.save(embed_path, embeddings)\n            print(f\"Saved embeddings to {embed_path}\")\n    \n    # Cosine similarity to centroid\n    cosine_sim = cosine_similarity(embeddings, centroid.reshape(1, -1)).flatten()\n    \n    # Anomaly score (Euclidean distance)\n    anomaly_score = np.linalg.norm(embeddings - centroid, axis=1)\n    \n    df['cosine_to_centroid'] = cosine_sim\n    df['anomaly_score'] = anomaly_score\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_feature_df(df, review_col='review', ngram_ref=None):\n    df = create_entropy_feature(df, review_col='review')\n    df = create_can_detect_feature(df, review_col='review')\n    df = create_alphabet_tag_feature(df, review_col='review')\n    df = create_word_and_char_counts_feature(df, review_col='review')\n    df = create_avg_word_length_feature(df, review_col='review')\n    df = create_repetition_feature(df, review_col='review')\n    df = create_punct_ratio_feature(df, review_col='review')\n    df = create_ngram_coherence_feature(df, ngram_ref, review_col='review')\n    #train_embed_path = 'train_embeddings.npy'\n    #test_embed_path = 'test_embeddings.npy'\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Training/Testing","metadata":{}},{"cell_type":"code","source":"# Create small sample to test the pipeline\nexperimental_df = pd.concat([merged_df.iloc[0:25, :], merged_df.iloc[-25:, :]], axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n#X = test_df.drop(columns=['gibberish'])\n#y = test_df['gibberish']\n\n# Step 1: Stratified split\ntrain_df, test_df = train_test_split(experimental_df, test_size=0.2, stratify=experimental_df['is_gibberish'], random_state=42)\nprint(f\"Train: {len(train_df)} rows, {train_df['is_gibberish'].sum()} gibberish\")\nprint(f\"Test: {len(test_df)} rows, {test_df['is_gibberish'].sum()} gibberish\")\n\n# Step 2: Build n-gram reference from training data\nngram_ref = build_ngram_reference(train_df['review'].dropna().tolist())\ncentroid = compute_centroid(train_df)\n\n# Step 3: Create feature DataFrames\ntrain_features = create_feature_df(train_df, ngram_ref=ngram_ref)\ntest_features = create_feature_df(test_df, ngram_ref=ngram_ref)\n\n# Save for later loading\ntrain_features.to_pickle('gibberish_train.pkl')\ntest_features.to_pickle('gibberish_test.pkl')\n\n# Step 4: Prepare X and y\nX_train = train_features.drop(columns=['review', 'is_gibberish'])\ny_train = train_features['is_gibberish']\nX_test = test_features.drop(columns=['review', 'is_gibberish'])\ny_test = test_features['is_gibberish']\n\n# Diagnostics\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {len(y_train)}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_test shape: {len(y_test)}\")\n\n# Save to reload later","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nimport pandas as pd\nimport numpy as np\nfrom langdetect import detect\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport re\n\n# Feature extraction function\ndef extract_features(text):\n    if not isinstance(text, str) or pd.isna(text):\n        text = \"\"\n    \n    # Entropy\n    length = len(text)\n    if length == 0:\n        entropy = 0\n    else:\n        char_counts = Counter(text.lower())\n        entropy = -sum((count/length) * np.log2(count/length) for count in char_counts.values())\n    \n    # Language\n    lang = detect(text) if length >= 3 else 'unknown'\n    is_unknown = 1 if lang == 'unknown' else 0\n    \n    # Word count and common words (example for English)\n    words = re.split(r'\\s+', text.strip())\n    word_count = len(words)\n    common_words = {'good', 'great', 'bad', 'course', 'learn'}  # Expand per language\n    common_word_ratio = sum(1 for w in words if w.lower() in common_words) / max(1, word_count)\n    \n    # Character length\n    char_length = len(text)\n    \n    # Repetition\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    \n    # Punctuation ratio\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    \n    return [entropy, is_unknown, word_count, common_word_ratio, char_length, max_repeats, punct_ratio]\n\n# Load labeled product review dataset\nproduct_df = pd.read_csv('product_reviews_labeled.csv')  # Assume columns: 'review', '\n\n# Extract features\nX = np.array([extract_features(review) for review in product_df['review']])\ny = product_df['is_gibberish'].values  # 1 = gibberish, 0 = not gibberish\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Apply to course reviews (merged_df)\ncourse_features = np.array([extract_features(review) for review in merged_df['reviews']])\ncourse_predictions = clf.predict(course_features)\nmerged_df['is_gibberish'] = course_predictions\n\n# Save results\nmerged_df.to_parquet('course_reviews_classified.parquet')\nprint(\"Course reviews classified and saved.\")\n''''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}