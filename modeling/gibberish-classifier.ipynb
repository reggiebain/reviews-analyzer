{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":856783,"sourceType":"datasetVersion","datasetId":271075}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gibberish Detector\n## \"Gibberish\" vs. \"Substantive\"\n- **Gibberish:** Random strings (e.g., \"asdfjkl\"), typos (\"jhkd\"), or meaningless repetition (\"aaa\").\n- **Substantive:** Coherent sentences with intent (e.g., \"Great course, learned a lot\" or \"这是一个很好的课程\").\n- **Edge Cases:** Short but valid reviews (e.g., \"Good\"), multilingual mixes, or sarcasm.","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers huggingface_hub langdetect pycountry","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:47:37.152662Z","iopub.execute_input":"2025-03-16T04:47:37.153085Z","iopub.status.idle":"2025-03-16T04:47:47.566679Z","shell.execute_reply.started":"2025-03-16T04:47:37.153037Z","shell.execute_reply":"2025-03-16T04:47:47.565228Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport scipy.stats as stats\nimport re\nimport nltk\nfrom nltk.corpus import words\nfrom collections import Counter\nimport math\nimport unicodedata\nimport time\nfrom tqdm import tqdm\nimport pickle\nfrom langdetect import detect\nimport warnings\nimport pycountry\nfrom scipy import stats\nfrom collections import defaultdict\nimport itertools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:47:47.568106Z","iopub.execute_input":"2025-03-16T04:47:47.568580Z","iopub.status.idle":"2025-03-16T04:47:54.732878Z","shell.execute_reply.started":"2025-03-16T04:47:47.568537Z","shell.execute_reply":"2025-03-16T04:47:54.731767Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"---------------------BEGIN DATA IMPORT------------","metadata":{}},{"cell_type":"markdown","source":"### Importing Data\n- The original data used an older character encoding\n- We'll save them as utf-8 so we can add the csv's to our project dataset","metadata":{}},{"cell_type":"code","source":"'''\n# Try different encodings\nfile_path = '/kaggle/input/gibberish-text-classification/Amazon.csv'  # Replace with your file path\ntry:\n    df = pd.read_csv(file_path, encoding='utf-8')  # Default, might fail\nexcept UnicodeDecodeError:\n    print(\"UTF-8 failed, trying other encodings...\")\n    try:\n        df = pd.read_csv(file_path, encoding='windows-1252')\n        print(\"Loaded with Windows-1252\")\n    except UnicodeDecodeError:\n        try:\n            df = pd.read_csv(file_path, encoding='iso-8859-1')\n            print(\"Loaded with ISO-8859-1\")\n        except UnicodeDecodeError:\n            try:\n                df = pd.read_csv(file_path, encoding='utf-16')\n                print(\"Loaded with UTF-16\")\n            except UnicodeDecodeError:\n                print(\"All common encodings failed. Check file encoding or corruption.\")\n'''                ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Load with Windows-1252, save as UTF-8 for adding to our custom kaggle dataset\ndef convert_encoding_type(file, out):\n    # Convert data to more universal encoding type\n    df = pd.read_csv(file, encoding='windows-1252')\n    df.to_csv(out, encoding='utf-8', index=False)\n    print(f\"Converted and saved as {out}\")\n    return\n\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Amazon.csv', 'amazon_reviews.csv')\nconvert_encoding_type('/kaggle/input/gibberish-text-classification/Gibberish.csv', 'amazon_gibberish.csv')\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------------END DATA CREATION---------------------\n","metadata":{}},{"cell_type":"markdown","source":"## EDA and Feature Extraction","metadata":{}},{"cell_type":"code","source":"reviews_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Amazon.csv', encoding='windows-1252', header=None, names=['rating', 'review'])\ngibb_df = pd.read_csv('/kaggle/input/gibberish-text-classification/Gibberish.csv', encoding='windows-1252')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:34.506318Z","iopub.execute_input":"2025-03-16T04:48:34.506785Z","iopub.status.idle":"2025-03-16T04:48:46.449896Z","shell.execute_reply.started":"2025-03-16T04:48:34.506748Z","shell.execute_reply":"2025-03-16T04:48:46.448640Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"According to the original data set at https://www.kaggle.com/datasets/bittlingmayer/amazonreviews, __label__2 means 4 and 5 star product reviews and label 1 is 1 and 2 start reviews. Looks like there's roughly equal amounts of both (positive and negative sentiment)","metadata":{}},{"cell_type":"code","source":"reviews_df.rating.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create sentiment column with 1 as positive (4/5 star), 0 negative (1,2 star)\nreviews_df['sentiment'] = (reviews_df['rating'] == '__label__2')*1\nreviews_df['is_gibberish'] = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.451840Z","iopub.execute_input":"2025-03-16T04:48:46.452172Z","iopub.status.idle":"2025-03-16T04:48:46.563025Z","shell.execute_reply.started":"2025-03-16T04:48:46.452144Z","shell.execute_reply":"2025-03-16T04:48:46.561924Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"There is a massive class imbalance with over 1 million real reviews and only 3767 gibberish reviews. We'll need to do some resampling.","metadata":{}},{"cell_type":"code","source":"gibb_df.rename(columns={'Response': 'review', 'Label': 'is_gibberish'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.565144Z","iopub.execute_input":"2025-03-16T04:48:46.565528Z","iopub.status.idle":"2025-03-16T04:48:46.576792Z","shell.execute_reply.started":"2025-03-16T04:48:46.565498Z","shell.execute_reply":"2025-03-16T04:48:46.575408Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Make new merged dataframe of just the reviews and whether they are gibberish\nmerged_df = pd.concat([gibb_df, reviews_df[['review', 'is_gibberish']]], ignore_index=True)\nmerged_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.578484Z","iopub.execute_input":"2025-03-16T04:48:46.578924Z","iopub.status.idle":"2025-03-16T04:48:46.723995Z","shell.execute_reply.started":"2025-03-16T04:48:46.578876Z","shell.execute_reply":"2025-03-16T04:48:46.722562Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                    review  is_gibberish\n0                                                      ggg             1\n1                                         hgghghghghghghhg             1\n2                              ufdhgjndfnvbhfdjvnjkmfgbdfg             1\n3                                                  dbdbdbd             1\n4                                                  dfgdfgd             1\n...                                                    ...           ...\n1052338  Cheap and flimsy: This was bought for an event...             0\n1052339  Total waste of money: This was a total waste o...             0\n1052340  Whitmor budget garment rack: I purchased the W...             0\n1052341  Serves its purpose: I bought this to put in my...             0\n1052342  Conversation on conversation: The so-called \"r...             0\n\n[1052343 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>is_gibberish</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ggg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hgghghghghghghhg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ufdhgjndfnvbhfdjvnjkmfgbdfg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dbdbdbd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dfgdfgd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1052338</th>\n      <td>Cheap and flimsy: This was bought for an event...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052339</th>\n      <td>Total waste of money: This was a total waste o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052340</th>\n      <td>Whitmor budget garment rack: I purchased the W...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052341</th>\n      <td>Serves its purpose: I bought this to put in my...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1052342</th>\n      <td>Conversation on conversation: The so-called \"r...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1052343 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"len(merged_df) - len(gibb_df) - len(reviews_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.724944Z","iopub.execute_input":"2025-03-16T04:48:46.725247Z","iopub.status.idle":"2025-03-16T04:48:46.732578Z","shell.execute_reply.started":"2025-03-16T04:48:46.725222Z","shell.execute_reply":"2025-03-16T04:48:46.730858Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Functions to Build Features","metadata":{}},{"cell_type":"code","source":"# FEATURE - Calculate entropy\ndef calculate_entropy(text):\n    \"\"\"Calculate Shannon entropy of the text to detect randomness.\"\"\"\n    if not text:\n        return 0\n    if not isinstance(text, str) or pd.isna(text):\n        return 0  # Return 0 for NaN or non-string values\n    text = str(text).lower()\n    length = len(text)\n    if length == 0:  # Handle empty strings\n        return 0\n    char_counts = Counter(text)\n    entropy = -sum((count/length) * math.log2(count/length) for count in char_counts.values())\n    return entropy\n\ndef create_entropy_feature(df, review_col='review'):\n    tqdm.pandas(desc='Calculating entropies: ')\n    df['entropy'] = df['review'].progress_apply(calculate_entropy)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.733887Z","iopub.execute_input":"2025-03-16T04:48:46.734408Z","iopub.status.idle":"2025-03-16T04:48:46.757201Z","shell.execute_reply.started":"2025-03-16T04:48:46.734365Z","shell.execute_reply":"2025-03-16T04:48:46.755931Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# FEATURE - Can detect language\nfrom langdetect import detect\nimport warnings\n\n# Suppress langdetect warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef detect_language(text):\n\n    if not isinstance(text, str) or pd.isna(text) or len(text.strip()) < 3:\n        return 'unknown'  # For NaN, empty, or very short text\n    try:\n        return detect(text)\n    except:\n        return 'unknown'  # Fallback for any detection errors\n\n# Returns 0 if we can't find the langauge, 1 if we can\ndef cannot_detect_language(text):\n    if text == 'unknown':\n        return 1\n    else:\n        return 0\n\ndef create_can_detect_feature(df, review_col='review'):\n    tqdm.pandas(desc=\"Detecting Language...\")\n    df['language'] = df[review_col].progress_apply(detect_language)\n    df['cannot_detect_language'] = df['language'].progress_apply(cannot_detect_language)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:46.758624Z","iopub.execute_input":"2025-03-16T04:48:46.759021Z","iopub.status.idle":"2025-03-16T04:48:46.787938Z","shell.execute_reply.started":"2025-03-16T04:48:46.758989Z","shell.execute_reply":"2025-03-16T04:48:46.786182Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"The next feature we'll look at is a flag of type of a few main alphabets. In our EDA we showed that outside of English, the most popular of our Coursera review languages mostly used a Latin or very close to Latin alphabet (somali, afrikaans, and tagalog for example use some variation of latin). We'll make flags for \n1. chinese\n2. cyrillic (for russian, slovenian)\n3. hangul (for korean)\n4. Latin or latin variant","metadata":{}},{"cell_type":"code","source":"# FEATURE - Type of Alphabet (run time 3 minutes)\ndef detect_alphabets(text):\n\n    if not isinstance(text, str) or not text:\n        return {\n            'Chinese': {'present': False, 'count': 0},\n            'Cyrillic': {'present': False, 'count': 0},\n            'Hangul': {'present': False, 'count': 0},\n            'Latin': {'present': False, 'count': 0}\n        }\n    \n    # Define Unicode ranges\n    ranges = {\n        'Chinese': (0x4E00, 0x9FFF),         # CJK Unified Ideographs\n        'Cyrillic': (0x0400, 0x04FF),       # Basic Cyrillic\n        'Hangul': (0xAC00, 0xD7AF),         # Hangul Syllables\n        'Latin': [(0x0000, 0x007F),         # Basic Latin\n                  (0x00A0, 0x00FF),         # Latin-1 Supplement\n                  (0x0100, 0x017F)],        # Latin Extended-A\n    }\n    \n    # Count characters per alphabet\n    alphabet_counts = defaultdict(int)\n    for char in text:\n        char_code = ord(char)\n        \n        # Check Chinese\n        if ranges['Chinese'][0] <= char_code <= ranges['Chinese'][1]:\n            alphabet_counts['Chinese'] += 1\n        \n        # Check Cyrillic\n        if ranges['Cyrillic'][0] <= char_code <= ranges['Cyrillic'][1]:\n            alphabet_counts['Cyrillic'] += 1\n        \n        # Check Hangul\n        if ranges['Hangul'][0] <= char_code <= ranges['Hangul'][1]:\n            alphabet_counts['Hangul'] += 1\n        \n        # Check Latin (multiple ranges)\n        for start, end in ranges['Latin']:\n            if start <= char_code <= end:\n                alphabet_counts['Latin'] += 1\n                break  # Stop after first match\n    \n    # Build result dictionary\n    result = {\n        'Chinese': {'present': alphabet_counts['Chinese'] > 0, 'count': alphabet_counts['Chinese']},\n        'Cyrillic': {'present': alphabet_counts['Cyrillic'] > 0, 'count': alphabet_counts['Cyrillic']},\n        'Abakada': {'present': alphabet_counts['Abakada'] > 0, 'count': alphabet_counts['Abakada']},\n        'Hangul': {'present': alphabet_counts['Hangul'] > 0, 'count': alphabet_counts['Hangul']},\n        'Latin': {'present': alphabet_counts['Latin'] > 0, 'count': alphabet_counts['Latin']}\n    }\n    \n    return result\n\ndef create_alphabet_tag_feature(df, review_col='review'):\n    # Create alphabets column with dictionary structure above\n    tqdm.pandas(desc='Creating alphabet tagging feature...')\n    df['alphabets'] = df[review_col].progress_apply(detect_alphabets)\n\n    # Create one-hot tag of different alphabets\n    df['has_chinese'] = df['alphabets'].apply(lambda x: x['Chinese']['present'])\n    df['has_cyrillic'] = df['alphabets'].apply(lambda x: x['Cyrillic']['present'])\n    df['has_abakada'] = df['alphabets'].apply(lambda x: x['Abakada']['present'])\n    df['has_hangul'] = df['alphabets'].apply(lambda x: x['Hangul']['present'])\n    df['has_latin'] = df['alphabets'].apply(lambda x: x['Latin']['present'])\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:47.282653Z","iopub.execute_input":"2025-03-16T04:48:47.283126Z","iopub.status.idle":"2025-03-16T04:48:47.295751Z","shell.execute_reply.started":"2025-03-16T04:48:47.283093Z","shell.execute_reply":"2025-03-16T04:48:47.294054Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 3x FEATURES - Total characters in review, word count of review, avg word length\ndef word_count(text):\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    return word_count\n\ndef char_count(text):\n    return len(text)\n\ndef create_word_and_char_counts_feature(df, review_col='review'):\n    tqdm.pandas(desc='Getting word/char counts...')\n    df['word_count'] = df[review_col].progress_apply(word_count)\n    df['n_chars'] = df[review_col].progress_apply(char_count)\n    return df\n\ndef get_avg_word_length(text):\n    # avg word length\n    words = re.split(f'\\s+', text.strip())\n    word_count = len(words)\n    avg_word_length = sum(len(word) for word in words if word) / max(1, word_count) if words else 0\n    return avg_word_length\n\ndef create_avg_word_length_feature(df, review_col='review'):\n    tqdm.pandas(desc='Getting avg word length feature...')\n    df['avg_word_length'] = df[review_col].progress_apply(get_avg_word_length)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:49.462630Z","iopub.execute_input":"2025-03-16T04:48:49.463069Z","iopub.status.idle":"2025-03-16T04:48:49.472564Z","shell.execute_reply.started":"2025-03-16T04:48:49.463032Z","shell.execute_reply":"2025-03-16T04:48:49.470596Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# FEATURE - Amount of Reptition\ndef get_max_repeated(text):\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    return max_repeats\ndef create_repetition_feature(df, review_col='review'):\n    tqdm.pandas(desc='Creating repetition feature...')\n    df['max_repeated'] = df[review_col].progress_apply(get_max_repeated)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:51.521936Z","iopub.execute_input":"2025-03-16T04:48:51.522497Z","iopub.status.idle":"2025-03-16T04:48:51.529101Z","shell.execute_reply.started":"2025-03-16T04:48:51.522453Z","shell.execute_reply":"2025-03-16T04:48:51.527568Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# FEATURE - Punctuation Ratio\ndef get_punct_ratio(text):\n    char_length = len(text)\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    return punct_ratio\ndef create_punct_ratio_feature(df, review_col='review'):\n    tqdm.pandas(desc='Creating punctuation ratio feature...')\n    df['punct_ratio'] = df[review_col].progress_apply(get_punct_ratio)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:51.716614Z","iopub.execute_input":"2025-03-16T04:48:51.717037Z","iopub.status.idle":"2025-03-16T04:48:51.722724Z","shell.execute_reply.started":"2025-03-16T04:48:51.717005Z","shell.execute_reply":"2025-03-16T04:48:51.721648Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# FEATURE - Contains common n-grams\n# Step 1 - Build n-gram reference from sample of real reviews\ndef build_ngram_reference(texts, n=2, top_k=1000, sample_size=10000):\n    \"\"\"\n    Build a set of common n-grams from a list of texts, assuming most are meaningful.\n    WARNING - Must only be done on training set\n    \n    Parameters:\n    - texts: List of text strings\n    - n: N-gram size\n    - top_k: Number of top n-grams to keep\n    - sample_size: Number of texts to sample (to speed up)\n    \n    Returns:\n    - Set of common n-grams\n    \"\"\"\n    # Sample texts to avoid over-processing (e.g., 1.19M reviews)\n    if len(texts) > sample_size:\n        texts = np.random.choice(texts, sample_size, replace=False)\n    \n    # Generate n-grams\n    ngrams = Counter()\n    for text in tqdm(texts, desc=\"Building n-gram reference...\"):\n        text = str(text).lower()\n        for i in range(len(text) - n + 1):\n            ngram = text[i:i+n]\n            if not ngram.isspace():\n                ngrams[ngram] += 1\n    \n    # Return top k most common n-grams\n    return set([ngram for ngram, _ in ngrams.most_common(top_k)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:53.047339Z","iopub.execute_input":"2025-03-16T04:48:53.047773Z","iopub.status.idle":"2025-03-16T04:48:53.055223Z","shell.execute_reply.started":"2025-03-16T04:48:53.047738Z","shell.execute_reply":"2025-03-16T04:48:53.053647Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# FEATURE - ngram coherence, fraction of ngrams that appear in list of common ngrams\ndef get_ngram_coherence(text, n=2):\n    text_lower = text.lower()\n    total_ngrams = max(1, len(text_lower) - n + 1)\n    valid_ngrams = sum(1 for i in range(total_ngrams) if text_lower[i:i+n] in ngram_ref)\n    ngram_coherence = valid_ngrams / total_ngrams\n    return ngram_coherence\n\ndef create_ngram_coherence_feature(df, ngram_ref, review_col='review'):\n    tqdm.pandas(desc='Calcualting ngram coherenece...')\n    df['ngram_coherence'] = df[review_col].progress_apply(get_ngram_coherence)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:48:55.267585Z","iopub.execute_input":"2025-03-16T04:48:55.267992Z","iopub.status.idle":"2025-03-16T04:48:55.274487Z","shell.execute_reply.started":"2025-03-16T04:48:55.267958Z","shell.execute_reply":"2025-03-16T04:48:55.272913Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import XLMRobertaTokenizer, XLMRobertaModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained XLM-R model and tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaModel.from_pretrained('xlm-roberta-base')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()\n\n# Function to get embeddings in batches\ndef get_embeddings(texts, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n        batch_texts = texts[i:i + batch_size]\n        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        # Use [CLS] token embedding (first token)\n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.append(batch_embeddings)\n    return np.vstack(embeddings)\n    \n# Compute centroid from substantive reviews\ndef compute_centroid(df, review_col='review', label_col='is_gibberish', sample_size=10000):\n    # Use real reviews (training data only)\n    real_texts = df[df[label_col] == 0][review_col].dropna().tolist()\n    if len(real_texts) > sample_size:\n        real_texts = np.random.choice(real_texts, sample_size, replace=False).tolist()\n    embeddings = get_embeddings(real_texts)\n    return np.mean(embeddings, axis=0)\n\n# Add embedding-based features\ndef add_embedding_features(df, centroid, review_col='review', embed_path=None):\n    if embed_path and os.path.exists(embed_path):\n        print(f\"Loading embeddings from {embed_path}\")\n        embeddings = np.load(embed_path)\n    else:\n        texts = df[review_col].fillna('').tolist()\n        embeddings = get_embeddings(texts)\n        if embed_path:\n            np.save(embed_path, embeddings)\n            print(f\"Saved embeddings to {embed_path}\")\n    \n    # Cosine similarity to centroid\n    cosine_sim = cosine_similarity(embeddings, centroid.reshape(1, -1)).flatten()\n    \n    # Anomaly score (Euclidean distance)\n    anomaly_score = np.linalg.norm(embeddings - centroid, axis=1)\n    \n    df['cosine_to_centroid'] = cosine_sim\n    df['anomaly_score'] = anomaly_score\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:51:51.654300Z","iopub.execute_input":"2025-03-16T04:51:51.654752Z","iopub.status.idle":"2025-03-16T04:51:55.190046Z","shell.execute_reply.started":"2025-03-16T04:51:51.654718Z","shell.execute_reply":"2025-03-16T04:51:55.188558Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def create_feature_df(df, review_col='review', ngram_ref=None, centroid=None):\n    df = create_entropy_feature(df, review_col='review')\n    df = create_can_detect_feature(df, review_col='review')\n    df = create_alphabet_tag_feature(df, review_col='review')\n    df = create_word_and_char_counts_feature(df, review_col='review')\n    df = create_avg_word_length_feature(df, review_col='review')\n    df = create_repetition_feature(df, review_col='review')\n    df = create_punct_ratio_feature(df, review_col='review')\n    df = create_ngram_coherence_feature(df, ngram_ref, review_col='review')\n    df = add_embedding_features(df, centroid)\n    #train_embed_path = 'train_embeddings.npy'\n    #test_embed_path = 'test_embeddings.npy'\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:51:55.191852Z","iopub.execute_input":"2025-03-16T04:51:55.192385Z","iopub.status.idle":"2025-03-16T04:51:55.201939Z","shell.execute_reply.started":"2025-03-16T04:51:55.192331Z","shell.execute_reply":"2025-03-16T04:51:55.199878Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Creating Training/Testing","metadata":{}},{"cell_type":"code","source":"# Create small sample to test the pipeline\nexperimental_df = pd.concat([merged_df.iloc[0:25, :], merged_df.iloc[-25:, :]], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:51:56.742730Z","iopub.execute_input":"2025-03-16T04:51:56.743112Z","iopub.status.idle":"2025-03-16T04:51:56.749868Z","shell.execute_reply.started":"2025-03-16T04:51:56.743085Z","shell.execute_reply":"2025-03-16T04:51:56.748356Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n#X = test_df.drop(columns=['gibberish'])\n#y = test_df['gibberish']\n\n# Step 1: Stratified split\ntrain_df, test_df = train_test_split(experimental_df, test_size=0.2, stratify=experimental_df['is_gibberish'], random_state=42)\nprint(f\"Train: {len(train_df)} rows, {train_df['is_gibberish'].sum()} gibberish\")\nprint(f\"Test: {len(test_df)} rows, {test_df['is_gibberish'].sum()} gibberish\")\n\n# Step 2: Build n-gram reference from training data\nngram_ref = build_ngram_reference(train_df['review'].dropna().tolist())\ncentroid = compute_centroid(train_df)\n\n# Step 3: Create feature DataFrames\ntrain_features = create_feature_df(train_df, ngram_ref=ngram_ref, centroid=centroid)\ntest_features = create_feature_df(test_df, ngram_ref=ngram_ref, centroid=centroid)\n\n# Save for later loading\ntrain_features.to_pickle('gibberish_train.pkl')\ntest_features.to_pickle('gibberish_test.pkl')\n\n# Step 4: Prepare X and y\nX_train = train_features.drop(columns=['review', 'is_gibberish'])\ny_train = train_features['is_gibberish']\nX_test = test_features.drop(columns=['review', 'is_gibberish'])\ny_test = test_features['is_gibberish']\n\n# Diagnostics\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {len(y_train)}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_test shape: {len(y_test)}\")\n\n# Save to reload later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:51:57.153521Z","iopub.execute_input":"2025-03-16T04:51:57.153918Z","iopub.status.idle":"2025-03-16T04:52:13.272609Z","shell.execute_reply.started":"2025-03-16T04:51:57.153887Z","shell.execute_reply":"2025-03-16T04:52:13.271542Z"}},"outputs":[{"name":"stdout","text":"Train: 40 rows, 20 gibberish\nTest: 10 rows, 5 gibberish\n","output_type":"stream"},{"name":"stderr","text":"Building n-gram reference...: 100%|██████████| 40/40 [00:00<00:00, 10966.22it/s]\nGenerating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.03s/it]\nCalculating entropies: 100%|██████████| 40/40 [00:00<00:00, 28605.65it/s]\nDetecting Language...: 100%|██████████| 40/40 [00:00<00:00, 63.23it/s]\nDetecting Language...: 100%|██████████| 40/40 [00:00<00:00, 83760.44it/s]\nCreating alphabet tagging feature...: 100%|██████████| 40/40 [00:00<00:00, 8175.63it/s]\nGetting word/char counts...: 100%|██████████| 40/40 [00:00<00:00, 30355.01it/s]\nGetting word/char counts...: 100%|██████████| 40/40 [00:00<00:00, 33968.85it/s]\nGetting avg word length feature...: 100%|██████████| 40/40 [00:00<00:00, 28373.44it/s]\nCreating repetition feature...: 100%|██████████| 40/40 [00:00<00:00, 8989.56it/s]\nCreating punctuation ratio feature...: 100%|██████████| 40/40 [00:00<00:00, 45136.44it/s]\nCalcualting ngram coherenece...: 100%|██████████| 40/40 [00:00<00:00, 16658.94it/s]\nGenerating embeddings: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\nCalculating entropies: 100%|██████████| 10/10 [00:00<00:00, 13828.90it/s]\nDetecting Language...: 100%|██████████| 10/10 [00:00<00:00, 86.03it/s]\nDetecting Language...: 100%|██████████| 10/10 [00:00<00:00, 6906.48it/s]\nCreating alphabet tagging feature...: 100%|██████████| 10/10 [00:00<00:00, 5180.71it/s]\nGetting word/char counts...: 100%|██████████| 10/10 [00:00<00:00, 14947.63it/s]\nGetting word/char counts...: 100%|██████████| 10/10 [00:00<00:00, 23537.06it/s]\nGetting avg word length feature...: 100%|██████████| 10/10 [00:00<00:00, 16500.02it/s]\nCreating repetition feature...: 100%|██████████| 10/10 [00:00<00:00, 6041.92it/s]\nCreating punctuation ratio feature...: 100%|██████████| 10/10 [00:00<00:00, 19719.34it/s]\nCalcualting ngram coherenece...: 100%|██████████| 10/10 [00:00<00:00, 9155.87it/s]\nGenerating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"X_train shape: (40, 17)\ny_train shape: 40\nX_test shape: (10, 17)\ny_test shape: 10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:52:21.017967Z","iopub.execute_input":"2025-03-16T04:52:21.018358Z","iopub.status.idle":"2025-03-16T04:52:21.050063Z","shell.execute_reply.started":"2025-03-16T04:52:21.018328Z","shell.execute_reply":"2025-03-16T04:52:21.048697Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                    review  is_gibberish  \\\n1052318  Very Helpful: This game guide helps. Alot! Gre...             0   \n1052326  Very dissapointed with colors.: I chose to by ...             0   \n14                               fe er ger ger gre ger ger             1   \n4                                                  dfgdfgd             1   \n1052333  smart, insightful, funny: A fabulous fast read...             0   \n\n          entropy language  cannot_detect_language  \\\n1052318  4.136835       en                       0   \n1052326  4.159115       en                       0   \n14       2.152629       da                       0   \n4        1.556657       cy                       0   \n1052333  4.282886       en                       0   \n\n                                                 alphabets  has_chinese  \\\n1052318  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n1052326  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n14       {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n4        {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n1052333  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n\n         has_cyrillic  has_abakada  has_hangul  has_latin  word_count  \\\n1052318         False        False       False       True          64   \n1052326         False        False       False       True          47   \n14              False        False       False       True           7   \n4               False        False       False       True           1   \n1052333         False        False       False       True          57   \n\n         n_chars  avg_word_length  max_repeated  punct_ratio  ngram_coherence  \\\n1052318      339         4.312500             2     0.017699              1.0   \n1052326      251         4.361702             3     0.027888              1.0   \n14            25         2.714286             1     0.000000              1.0   \n4              7         7.000000             1     0.000000              1.0   \n1052333      367         5.456140             2     0.021798              1.0   \n\n         cosine_to_centroid  anomaly_score  \n1052318            0.999379       0.661312  \n1052326            0.998125       1.154295  \n14                 0.996776       1.505363  \n4                  0.995700       1.738235  \n1052333            0.998280       1.106473  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>is_gibberish</th>\n      <th>entropy</th>\n      <th>language</th>\n      <th>cannot_detect_language</th>\n      <th>alphabets</th>\n      <th>has_chinese</th>\n      <th>has_cyrillic</th>\n      <th>has_abakada</th>\n      <th>has_hangul</th>\n      <th>has_latin</th>\n      <th>word_count</th>\n      <th>n_chars</th>\n      <th>avg_word_length</th>\n      <th>max_repeated</th>\n      <th>punct_ratio</th>\n      <th>ngram_coherence</th>\n      <th>cosine_to_centroid</th>\n      <th>anomaly_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1052318</th>\n      <td>Very Helpful: This game guide helps. Alot! Gre...</td>\n      <td>0</td>\n      <td>4.136835</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>64</td>\n      <td>339</td>\n      <td>4.312500</td>\n      <td>2</td>\n      <td>0.017699</td>\n      <td>1.0</td>\n      <td>0.999379</td>\n      <td>0.661312</td>\n    </tr>\n    <tr>\n      <th>1052326</th>\n      <td>Very dissapointed with colors.: I chose to by ...</td>\n      <td>0</td>\n      <td>4.159115</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>47</td>\n      <td>251</td>\n      <td>4.361702</td>\n      <td>3</td>\n      <td>0.027888</td>\n      <td>1.0</td>\n      <td>0.998125</td>\n      <td>1.154295</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>fe er ger ger gre ger ger</td>\n      <td>1</td>\n      <td>2.152629</td>\n      <td>da</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>7</td>\n      <td>25</td>\n      <td>2.714286</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.996776</td>\n      <td>1.505363</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dfgdfgd</td>\n      <td>1</td>\n      <td>1.556657</td>\n      <td>cy</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7.000000</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.995700</td>\n      <td>1.738235</td>\n    </tr>\n    <tr>\n      <th>1052333</th>\n      <td>smart, insightful, funny: A fabulous fast read...</td>\n      <td>0</td>\n      <td>4.282886</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>57</td>\n      <td>367</td>\n      <td>5.456140</td>\n      <td>2</td>\n      <td>0.021798</td>\n      <td>1.0</td>\n      <td>0.998280</td>\n      <td>1.106473</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:52:21.467406Z","iopub.execute_input":"2025-03-16T04:52:21.467840Z","iopub.status.idle":"2025-03-16T04:52:21.499058Z","shell.execute_reply.started":"2025-03-16T04:52:21.467770Z","shell.execute_reply":"2025-03-16T04:52:21.497693Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                    review  is_gibberish  \\\n1052328  Shocking results ... not the good kind mind yo...             0   \n1052324  Do not buy this one: I end up bought another f...             0   \n1052332  I keep going back to this gem: The Pawnshop Ch...             0   \n1052337  Good book: We like having a strange assortment...             0   \n9                                  ddddddddddddddddddddddd             1   \n\n          entropy language  cannot_detect_language  \\\n1052328  4.284819       en                       0   \n1052324  4.289984       en                       0   \n1052332  4.165865       en                       0   \n1052337  4.030339       en                       0   \n9       -0.000000       cy                       0   \n\n                                                 alphabets  has_chinese  \\\n1052328  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n1052324  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n1052332  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n1052337  {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n9        {'Chinese': {'present': False, 'count': 0}, 'C...        False   \n\n         has_cyrillic  has_abakada  has_hangul  has_latin  word_count  \\\n1052328         False        False       False       True         121   \n1052324         False        False       False       True          44   \n1052332         False        False       False       True          65   \n1052337         False        False       False       True          23   \n9               False        False       False       True           1   \n\n         n_chars  avg_word_length  max_repeated  punct_ratio  ngram_coherence  \\\n1052328      669         4.537190             3     0.031390         0.985030   \n1052324      239         4.454545             3     0.008368         0.962185   \n1052332      381         4.876923             2     0.013123         0.984211   \n1052337      132         4.782609             2     0.015152         1.000000   \n9             23        23.000000            23     0.000000         0.000000   \n\n         cosine_to_centroid  anomaly_score  \n1052328            0.999349       0.678035  \n1052324            0.999036       0.823802  \n1052332            0.999580       0.543829  \n1052337            0.999458       0.618045  \n9                  0.996597       1.546895  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>is_gibberish</th>\n      <th>entropy</th>\n      <th>language</th>\n      <th>cannot_detect_language</th>\n      <th>alphabets</th>\n      <th>has_chinese</th>\n      <th>has_cyrillic</th>\n      <th>has_abakada</th>\n      <th>has_hangul</th>\n      <th>has_latin</th>\n      <th>word_count</th>\n      <th>n_chars</th>\n      <th>avg_word_length</th>\n      <th>max_repeated</th>\n      <th>punct_ratio</th>\n      <th>ngram_coherence</th>\n      <th>cosine_to_centroid</th>\n      <th>anomaly_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1052328</th>\n      <td>Shocking results ... not the good kind mind yo...</td>\n      <td>0</td>\n      <td>4.284819</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>121</td>\n      <td>669</td>\n      <td>4.537190</td>\n      <td>3</td>\n      <td>0.031390</td>\n      <td>0.985030</td>\n      <td>0.999349</td>\n      <td>0.678035</td>\n    </tr>\n    <tr>\n      <th>1052324</th>\n      <td>Do not buy this one: I end up bought another f...</td>\n      <td>0</td>\n      <td>4.289984</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>44</td>\n      <td>239</td>\n      <td>4.454545</td>\n      <td>3</td>\n      <td>0.008368</td>\n      <td>0.962185</td>\n      <td>0.999036</td>\n      <td>0.823802</td>\n    </tr>\n    <tr>\n      <th>1052332</th>\n      <td>I keep going back to this gem: The Pawnshop Ch...</td>\n      <td>0</td>\n      <td>4.165865</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>65</td>\n      <td>381</td>\n      <td>4.876923</td>\n      <td>2</td>\n      <td>0.013123</td>\n      <td>0.984211</td>\n      <td>0.999580</td>\n      <td>0.543829</td>\n    </tr>\n    <tr>\n      <th>1052337</th>\n      <td>Good book: We like having a strange assortment...</td>\n      <td>0</td>\n      <td>4.030339</td>\n      <td>en</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>23</td>\n      <td>132</td>\n      <td>4.782609</td>\n      <td>2</td>\n      <td>0.015152</td>\n      <td>1.000000</td>\n      <td>0.999458</td>\n      <td>0.618045</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ddddddddddddddddddddddd</td>\n      <td>1</td>\n      <td>-0.000000</td>\n      <td>cy</td>\n      <td>0</td>\n      <td>{'Chinese': {'present': False, 'count': 0}, 'C...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n      <td>23</td>\n      <td>23.000000</td>\n      <td>23</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.996597</td>\n      <td>1.546895</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"'''\nimport pandas as pd\nimport numpy as np\nfrom langdetect import detect\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport re\n\n# Feature extraction function\ndef extract_features(text):\n    if not isinstance(text, str) or pd.isna(text):\n        text = \"\"\n    \n    # Entropy\n    length = len(text)\n    if length == 0:\n        entropy = 0\n    else:\n        char_counts = Counter(text.lower())\n        entropy = -sum((count/length) * np.log2(count/length) for count in char_counts.values())\n    \n    # Language\n    lang = detect(text) if length >= 3 else 'unknown'\n    is_unknown = 1 if lang == 'unknown' else 0\n    \n    # Word count and common words (example for English)\n    words = re.split(r'\\s+', text.strip())\n    word_count = len(words)\n    common_words = {'good', 'great', 'bad', 'course', 'learn'}  # Expand per language\n    common_word_ratio = sum(1 for w in words if w.lower() in common_words) / max(1, word_count)\n    \n    # Character length\n    char_length = len(text)\n    \n    # Repetition\n    max_repeats = max([sum(1 for _ in g) for _, g in itertools.groupby(text)] or [0])\n    \n    # Punctuation ratio\n    punct_count = sum(1 for c in text if c in '.,!?')\n    punct_ratio = punct_count / max(1, char_length)\n    \n    return [entropy, is_unknown, word_count, common_word_ratio, char_length, max_repeats, punct_ratio]\n\n# Load labeled product review dataset\nproduct_df = pd.read_csv('product_reviews_labeled.csv')  # Assume columns: 'review', '\n\n# Extract features\nX = np.array([extract_features(review) for review in product_df['review']])\ny = product_df['is_gibberish'].values  # 1 = gibberish, 0 = not gibberish\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Apply to course reviews (merged_df)\ncourse_features = np.array([extract_features(review) for review in merged_df['reviews']])\ncourse_predictions = clf.predict(course_features)\nmerged_df['is_gibberish'] = course_predictions\n\n# Save results\nmerged_df.to_parquet('course_reviews_classified.parquet')\nprint(\"Course reviews classified and saved.\")\n''''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}